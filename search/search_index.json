{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Strona g\u0142\u00f3wna","text":"<p>Witam Pa\u0144stwa na zaj\u0119ciach z przedmiotu Analiza danych w naukach o Ziemi dla kierunku Geoinformatyka. Poniewa\u017c jest to kurs na studiach magisterskich, chcia\u0142bym, \u017ceby zaj\u0119cie by\u0142y w du\u017cej mierze projektowe.</p>"},{"location":"#informacje-o-przedmocie","title":"\ud83d\udccc Informacje o przedmocie","text":"<ul> <li>Prowadz\u0105cy: Jakub Staszel</li> <li>Forma zaj\u0119\u0107: \u0107wiczenia laboratoryjne</li> <li>Liczba godzin: 1h 30 min x 30</li> <li>Miejsce: Sala 313, A0</li> </ul>"},{"location":"#cel-kursu","title":"\ud83c\udfaf Cel kursu","text":"<p>Po uko\u0144czeniu kursu uczestnicy b\u0119d\u0105 potrafili:</p> <ul> <li>Wykorzystywa\u0107 Python do analizy i przetwarzania danych przestrzennych,</li> <li>Korzysta\u0107 z bibliotek takich jak GeoPandas, Rasterio, Fiona czy Shapely,</li> <li>Tworzy\u0107 interaktywne mapy przy u\u017cyciu Folium i Streamlit,</li> <li>U\u017cywa\u0107 Dask do przetwarzania r\u00f3wnoleg\u0142ego.</li> </ul>"},{"location":"#forma-zaliczenia","title":"\ud83d\udcdd Forma zaliczenia","text":"<p>Aby zaliczy\u0107 kurs, uczestnicy musz\u0105:</p> <ol> <li>Wykona\u0107 projekt semestralny,</li> <li>Doda\u0107 dokumentacj\u0119 projektu w MkDocs,</li> <li>Przedstawi\u0107 dzia\u0142aj\u0105cy projekt na ostatnich zaj\u0119ciach.</li> </ol>"},{"location":"#projekty","title":"\ud83d\ude80 Projekty","text":"<ul> <li>Tematy projekt\u00f3w zostan\u0105 podane w p\u00f3\u017aniejszym terminie,</li> <li>Projekty b\u0119d\u0105 realizowane w grupach 2 lub 3 osobowych,</li> <li>Je\u015bli kto\u015b ma w\u0142asny pomys\u0142 na projekt, mo\u017cna go zrealizowa\u0107 w ramach tych zaj\u0119\u0107.</li> </ul>"},{"location":"#wymagania","title":"\ud83d\udee0\ufe0f Wymagania","text":"<ul> <li>Znajomo\u015b\u0107 podstaw Python, GIS i git,</li> <li>Konto na GitHub.</li> </ul>"},{"location":"#kontakt","title":"\ud83d\udce2 Kontakt","text":"<p>W razie pyta\u0144, prosz\u0119 o kontakt na Teams lub poprzez email - jstaszel@agh.edu.pl.</p>"},{"location":"dask/","title":"Dask","text":"In\u00a0[1]: Copied! <pre>from osgeo import gdal\nimport dask.array as da\nimport rioxarray as rxr\nimport dask\nimport matplotlib.pyplot as plt\nfrom dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler\nfrom dask.distributed import Client\nimport pystac_client\n</pre> from osgeo import gdal import dask.array as da import rioxarray as rxr import dask import matplotlib.pyplot as plt from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler from dask.distributed import Client import pystac_client In\u00a0[2]: Copied! <pre># Wyszukiwanie w katalogu STAC\nstac_url = \"https://earth-search.aws.element84.com/v1\"\nstac_client = pystac_client.Client.open(stac_url)\n\n# Filtrujemy obrazy Sentinel-2 na podstawie lokalizacji i daty\nsearch = stac_client.search(\n    collections=[\"sentinel-2-c1-l2a\"],\n    bbox=[69.5, 34.0, 70.5, 35.0],  # Przyk\u0142adowy bbox (Afganistan, zmie\u0144 na swoje wsp\u00f3\u0142rz\u0119dne)\n    datetime=\"2023-01-06T00:00:00Z/2023-06-22T23:59:59Z\",\n    max_items=1\n)\n\nitem = next(search.items(), None)\nif not item:\n    raise ValueError(\"Nie znaleziono pasuj\u0105cych obraz\u00f3w Sentinel-2 w STAC API.\")\n</pre> # Wyszukiwanie w katalogu STAC stac_url = \"https://earth-search.aws.element84.com/v1\" stac_client = pystac_client.Client.open(stac_url)  # Filtrujemy obrazy Sentinel-2 na podstawie lokalizacji i daty search = stac_client.search(     collections=[\"sentinel-2-c1-l2a\"],     bbox=[69.5, 34.0, 70.5, 35.0],  # Przyk\u0142adowy bbox (Afganistan, zmie\u0144 na swoje wsp\u00f3\u0142rz\u0119dne)     datetime=\"2023-01-06T00:00:00Z/2023-06-22T23:59:59Z\",     max_items=1 )  item = next(search.items(), None) if not item:     raise ValueError(\"Nie znaleziono pasuj\u0105cych obraz\u00f3w Sentinel-2 w STAC API.\")  In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import LocalCluster\ncluster = LocalCluster(n_workers=2, threads_per_worker=1, memory_limit=\"1GB\")\nclient = Client(cluster)\nclient.get_worker_logs()  # Sprawdzenie log\u00f3w worker\u00f3w\ndisplay(client)  # Wy\u015bwietla link do dashboardu\n</pre> from dask.distributed import LocalCluster cluster = LocalCluster(n_workers=2, threads_per_worker=1, memory_limit=\"1GB\") client = Client(cluster) client.get_worker_logs()  # Sprawdzenie log\u00f3w worker\u00f3w display(client)  # Wy\u015bwietla link do dashboardu Client <p>Client-da49de19-00e9-11f0-a1cc-60a5e265b32c</p> Connection method: Cluster object Cluster type: distributed.LocalCluster Dashboard:  http://127.0.0.1:8787/status Cluster Info LocalCluster <p>dc93f438</p> Dashboard: http://127.0.0.1:8787/status Workers: 2                  Total threads: 2                  Total memory: 1.86 GiB                  Status: running Using processes: True Scheduler Info Scheduler <p>Scheduler-0f5de4c6-1ed4-4508-b4f7-127f590edd7c</p> Comm: tcp://127.0.0.1:63106                      Workers: 2                      Dashboard: http://127.0.0.1:8787/status Total threads: 2                      Started: Just now                      Total memory: 1.86 GiB                      Workers Worker: 0 Comm:  tcp://127.0.0.1:63118                          Total threads:  1                          Dashboard:  http://127.0.0.1:63120/status Memory:  0.93 GiB                          Nanny:  tcp://127.0.0.1:63109                          Local directory:  C:\\Users\\JAS~1.SPY\\AppData\\Local\\Temp\\dask-scratch-space\\worker-3c5ozg8a                          Worker: 1 Comm:  tcp://127.0.0.1:63119                          Total threads:  1                          Dashboard:  http://127.0.0.1:63122/status Memory:  0.93 GiB                          Nanny:  tcp://127.0.0.1:63111                          Local directory:  C:\\Users\\JAS~1.SPY\\AppData\\Local\\Temp\\dask-scratch-space\\worker-ejlwhtsl                          <pre>2025-03-14 18:14:54,471 - distributed.scheduler - WARNING - Worker failed to heartbeat for 828s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:63118', name: 0, status: running, memory: 0, processing: 0&gt;\n2025-03-14 18:14:54,493 - distributed.scheduler - WARNING - Worker failed to heartbeat for 828s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:63119', name: 1, status: running, memory: 0, processing: 0&gt;\n2025-03-14 18:14:55,974 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 18:14:56,006 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 20:17:53,366 - distributed.scheduler - WARNING - Worker failed to heartbeat for 7193s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:59107', name: 0, status: running, memory: 0, processing: 0&gt;\n2025-03-14 20:17:53,369 - distributed.scheduler - WARNING - Worker failed to heartbeat for 7193s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:59108', name: 1, status: running, memory: 0, processing: 0&gt;\n2025-03-14 20:17:54,916 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 20:17:54,939 - distributed.nanny - WARNING - Restarting worker\n2025-03-17 07:02:47,548 - distributed.scheduler - WARNING - Worker failed to heartbeat for 211473s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:54541', name: 1, status: running, memory: 0, processing: 0&gt;\n2025-03-17 07:02:47,549 - distributed.scheduler - WARNING - Worker failed to heartbeat for 211473s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:54542', name: 0, status: running, memory: 0, processing: 0&gt;\n2025-03-17 07:02:49,676 - distributed.nanny - WARNING - Restarting worker\n2025-03-17 07:02:49,711 - distributed.nanny - WARNING - Restarting worker\n</pre> In\u00a0[4]: Copied! <pre># Pobranie linku do pasma czerwonego (B04)\ncog_url = item.assets[\"red\"].href\n\nimport rasterio\nfrom rasterio.session import AWSSession\nimport boto3\n\n# Pobierz plik do lokalnego cache\nsession = AWSSession(boto3.Session(), requester_pays=True)\nwith rasterio.Env(session):\n    raster = rxr.open_rasterio(cog_url, masked=False, chunks=\"auto\", overview_level=0, session=session)\nraster = raster.squeeze(dim=\"band\")  # Usuwa wymiar \"band\" je\u015bli ma tylko jeden element\nraster = raster.chunk({\"x\": 512, \"y\": 512}).astype(\"float32\")\n</pre>  # Pobranie linku do pasma czerwonego (B04) cog_url = item.assets[\"red\"].href  import rasterio from rasterio.session import AWSSession import boto3  # Pobierz plik do lokalnego cache session = AWSSession(boto3.Session(), requester_pays=True) with rasterio.Env(session):     raster = rxr.open_rasterio(cog_url, masked=False, chunks=\"auto\", overview_level=0, session=session) raster = raster.squeeze(dim=\"band\")  # Usuwa wymiar \"band\" je\u015bli ma tylko jeden element raster = raster.chunk({\"x\": 512, \"y\": 512}).astype(\"float32\") In\u00a0[5]: Copied! <pre>raster\n</pre> raster Out[5]: <pre>&lt;xarray.DataArray (y: 5490, x: 5490)&gt; Size: 121MB\ndask.array&lt;astype, shape=(5490, 5490), dtype=float32, chunksize=(512, 512), chunktype=numpy.ndarray&gt;\nCoordinates:\n    band         int64 8B 1\n  * x            (x) float64 44kB 5e+05 5e+05 5e+05 ... 6.098e+05 6.098e+05\n  * y            (y) float64 44kB 3.8e+06 3.8e+06 3.8e+06 ... 3.69e+06 3.69e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    OVR_RESAMPLING_ALG:  AVERAGE\n    AREA_OR_POINT:       Area\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0</pre>xarray.DataArray<ul><li>y: 5490</li><li>x: 5490</li></ul><ul><li>dask.array&lt;chunksize=(512, 512), meta=np.ndarray&gt;  Array   Chunk   Bytes   114.98 MiB   1.00 MiB   Shape   (5490, 5490)   (512, 512)   Dask graph   121 chunks in 5 graph layers   Data type   float32 numpy.ndarray  5490 5490 </li><li>Coordinates: (4)<ul><li>band()int641<pre>array(1)</pre></li><li>x(x)float645e+05 5e+05 ... 6.098e+05 6.098e+05<pre>array([499990., 500010., 500030., ..., 609730., 609750., 609770.],\n      shape=(5490,))</pre></li><li>y(y)float643.8e+06 3.8e+06 ... 3.69e+06<pre>array([3800030., 3800010., 3799990., ..., 3690290., 3690270., 3690250.],\n      shape=(5490,))</pre></li><li>spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 42N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",69],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32642\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 42Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :69.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 42N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",69],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32642\"]]GeoTransform :499980.0 20.0 0.0 3800040.0 0.0 -20.0<pre>array(0)</pre></li></ul></li><li>Indexes: (2)<ul><li>xPandasIndex<pre>PandasIndex(Index([499990.0, 500010.0, 500030.0, 500050.0, 500070.0, 500090.0, 500110.0,\n       500130.0, 500150.0, 500170.0,\n       ...\n       609590.0, 609610.0, 609630.0, 609650.0, 609670.0, 609690.0, 609710.0,\n       609730.0, 609750.0, 609770.0],\n      dtype='float64', name='x', length=5490))</pre></li><li>yPandasIndex<pre>PandasIndex(Index([3800030.0, 3800010.0, 3799990.0, 3799970.0, 3799950.0, 3799930.0,\n       3799910.0, 3799890.0, 3799870.0, 3799850.0,\n       ...\n       3690430.0, 3690410.0, 3690390.0, 3690370.0, 3690350.0, 3690330.0,\n       3690310.0, 3690290.0, 3690270.0, 3690250.0],\n      dtype='float64', name='y', length=5490))</pre></li></ul></li><li>Attributes: (5)OVR_RESAMPLING_ALG :AVERAGEAREA_OR_POINT :Area_FillValue :0scale_factor :1.0add_offset :0.0</li></ul> In\u00a0[6]: Copied! <pre>workers_info = client.scheduler_info()[\"workers\"]\nprint(\"Lista worker\u00f3w Dask:\")\nfor worker, info in workers_info.items():\n    memory_used = info.get(\"metrics\", {}).get(\"memory\", \"Brak danych\")\n    nthreads = info.get(\"nthreads\", \"Brak danych\")\n    print(f\"Worker: {worker}, Pami\u0119\u0107: {memory_used} B, Aktywne w\u0105tki: {nthreads}\")\n</pre> workers_info = client.scheduler_info()[\"workers\"] print(\"Lista worker\u00f3w Dask:\") for worker, info in workers_info.items():     memory_used = info.get(\"metrics\", {}).get(\"memory\", \"Brak danych\")     nthreads = info.get(\"nthreads\", \"Brak danych\")     print(f\"Worker: {worker}, Pami\u0119\u0107: {memory_used} B, Aktywne w\u0105tki: {nthreads}\") <pre>Lista worker\u00f3w Dask:\nWorker: tcp://127.0.0.1:63118, Pami\u0119\u0107: 121352192 B, Aktywne w\u0105tki: 1\nWorker: tcp://127.0.0.1:63119, Pami\u0119\u0107: 121106432 B, Aktywne w\u0105tki: 1\n</pre> In\u00a0[7]: Copied! <pre>from normalize import normalize_xr\n</pre> from normalize import normalize_xr In\u00a0[71]: Copied! <pre>import xarray as xr\n\ndef normalize_xr(data):\n    min_val = data.min()\n    max_val = data.max()\n    return (data - min_val) / (max_val - min_val)\n</pre> import xarray as xr  def normalize_xr(data):     min_val = data.min()     max_val = data.max()     return (data - min_val) / (max_val - min_val) In\u00a0[9]: Copied! <pre>import xarray as xr\n</pre> import xarray as xr  In\u00a0[10]: Copied! <pre>normalized_raster = xr.apply_ufunc(normalize_xr, raster, dask='parallelized', output_dtypes=[raster.dtype])\n</pre> normalized_raster = xr.apply_ufunc(normalize_xr, raster, dask='parallelized', output_dtypes=[raster.dtype]) In\u00a0[12]: Copied! <pre>raster = raster.compute()  # Pobiera dane do RAM przed dalszym przetwarzaniem\n\nnormalized_raster = (raster - raster.min()) / (raster.max() - raster.min())\n</pre> raster = raster.compute()  # Pobiera dane do RAM przed dalszym przetwarzaniem  normalized_raster = (raster - raster.min()) / (raster.max() - raster.min()) <pre>2025-03-14 16:36:35,708 - distributed.protocol.pickle - ERROR - Failed to serialize &lt;ToPickle: HighLevelGraph with 1 layers.\n&lt;dask.highlevelgraph.HighLevelGraph object at 0x1f3de6e8650&gt;\n 0. 2146920665792\n&gt;.\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 60, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 65, in dumps\n    pickler.dump(x)\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 77, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py\", line 1537, in dumps\n    cp.dump(obj)\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py\", line 1303, in dump\n    return super().dump(obj)\n           ^^^^^^^^^^^^^^^^^\n_pickle.PicklingError: Can't pickle &lt;built-in function input&gt;: it's not the same object as builtins.input\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:60, in dumps(x, buffer_callback, protocol)\n     59 try:\n---&gt; 60     result = pickle.dumps(x, **dump_kwargs)\n     61 except Exception:\n\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:65, in dumps(x, buffer_callback, protocol)\n     64 buffers.clear()\n---&gt; 65 pickler.dump(x)\n     66 result = f.getvalue()\n\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nPicklingError                             Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:366, in serialize(x, serializers, on_error, context, iterate_collection)\n    365 try:\n--&gt; 366     header, frames = dumps(x, context=context) if wants_context else dumps(x)\n    367     header[\"serializer\"] = name\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:78, in pickle_dumps(x, context)\n     76     writeable.append(not f.readonly)\n---&gt; 78 frames[0] = pickle.dumps(\n     79     x,\n     80     buffer_callback=buffer_callback,\n     81     protocol=context.get(\"pickle-protocol\", None) if context else None,\n     82 )\n     83 header = {\n     84     \"serializer\": \"pickle\",\n     85     \"writeable\": tuple(writeable),\n     86 }\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:77, in dumps(x, buffer_callback, protocol)\n     76     buffers.clear()\n---&gt; 77     result = cloudpickle.dumps(x, **dump_kwargs)\n     78 except Exception:\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1537, in dumps(obj, protocol, buffer_callback)\n   1536 cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)\n-&gt; 1537 cp.dump(obj)\n   1538 return file.getvalue()\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1303, in Pickler.dump(self, obj)\n   1302 try:\n-&gt; 1303     return super().dump(obj)\n   1304 except RuntimeError as e:\n\nPicklingError: Can't pickle &lt;built-in function input&gt;: it's not the same object as builtins.input\n\nThe above exception was the direct cause of the following exception:\n\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 raster = raster.compute()  # Pobiera dane do RAM przed dalszym przetwarzaniem\n      3 normalized_raster = (raster - raster.min()) / (raster.max() - raster.min())\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataarray.py:1206, in DataArray.compute(self, **kwargs)\n   1181 \"\"\"Manually trigger loading of this array's data from disk or a\n   1182 remote source into memory and return a new array.\n   1183 \n   (...)   1203 dask.compute\n   1204 \"\"\"\n   1205 new = self.copy(deep=False)\n-&gt; 1206 return new.load(**kwargs)\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataarray.py:1174, in DataArray.load(self, **kwargs)\n   1154 def load(self, **kwargs) -&gt; Self:\n   1155     \"\"\"Manually trigger loading of this array's data from disk or a\n   1156     remote source into memory and return this array.\n   1157 \n   (...)   1172     dask.compute\n   1173     \"\"\"\n-&gt; 1174     ds = self._to_temp_dataset().load(**kwargs)\n   1175     new = self._from_temp_dataset(ds)\n   1176     self._variable = new._variable\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataset.py:900, in Dataset.load(self, **kwargs)\n    897 chunkmanager = get_chunked_array_type(*lazy_data.values())\n    899 # evaluate all the chunked arrays simultaneously\n--&gt; 900 evaluated_data: tuple[np.ndarray[Any, Any], ...] = chunkmanager.compute(\n    901     *lazy_data.values(), **kwargs\n    902 )\n    904 for k, data in zip(lazy_data, evaluated_data, strict=False):\n    905     self.variables[k].data = data\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:85, in DaskManager.compute(self, *data, **kwargs)\n     80 def compute(\n     81     self, *data: Any, **kwargs: Any\n     82 ) -&gt; tuple[np.ndarray[Any, _DType_co], ...]:\n     83     from dask.array import compute\n---&gt; 85     return compute(*data, **kwargs)\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\dask\\base.py:662, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    659     postcomputes.append(x.__dask_postcompute__())\n    661 with shorten_traceback():\n--&gt; 662     results = schedule(dsk, keys, **kwargs)\n    664 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:392, in serialize(x, serializers, on_error, context, iterate_collection)\n    390     except Exception:\n    391         raise TypeError(msg) from exc\n--&gt; 392     raise TypeError(msg, str_x) from exc\n    393 else:  # pragma: nocover\n    394     raise ValueError(f\"{on_error=}; expected 'message' or 'raise'\")\n\nTypeError: ('Could not serialize object of type HighLevelGraph', '&lt;ToPickle: HighLevelGraph with 1 layers.\\n&lt;dask.highlevelgraph.HighLevelGraph object at 0x1f3de6e8650&gt;\\n 0. 2146920665792\\n&gt;')</pre> In\u00a0[11]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 6))\n\nwith Profiler() as prof, ResourceProfiler() as rprof, CacheProfiler() as cprof, ProgressBar():\n    result = normalized_raster.compute()\n</pre> fig, ax = plt.subplots(figsize=(8, 6))  with Profiler() as prof, ResourceProfiler() as rprof, CacheProfiler() as cprof, ProgressBar():     result = normalized_raster.compute() <pre>2025-03-14 16:35:21,121 - distributed.protocol.pickle - ERROR - Failed to serialize &lt;ToPickle: HighLevelGraph with 1 layers.\n&lt;dask.highlevelgraph.HighLevelGraph object at 0x1f3dbc91f90&gt;\n 0. 2146875929152\n&gt;.\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 60, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 65, in dumps\n    pickler.dump(x)\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 77, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py\", line 1537, in dumps\n    cp.dump(obj)\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py\", line 1303, in dump\n    return super().dump(obj)\n           ^^^^^^^^^^^^^^^^^\n_pickle.PicklingError: Can't pickle &lt;built-in function input&gt;: it's not the same object as builtins.input\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:60, in dumps(x, buffer_callback, protocol)\n     59 try:\n---&gt; 60     result = pickle.dumps(x, **dump_kwargs)\n     61 except Exception:\n\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:65, in dumps(x, buffer_callback, protocol)\n     64 buffers.clear()\n---&gt; 65 pickler.dump(x)\n     66 result = f.getvalue()\n\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nPicklingError                             Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:366, in serialize(x, serializers, on_error, context, iterate_collection)\n    365 try:\n--&gt; 366     header, frames = dumps(x, context=context) if wants_context else dumps(x)\n    367     header[\"serializer\"] = name\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:78, in pickle_dumps(x, context)\n     76     writeable.append(not f.readonly)\n---&gt; 78 frames[0] = pickle.dumps(\n     79     x,\n     80     buffer_callback=buffer_callback,\n     81     protocol=context.get(\"pickle-protocol\", None) if context else None,\n     82 )\n     83 header = {\n     84     \"serializer\": \"pickle\",\n     85     \"writeable\": tuple(writeable),\n     86 }\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:77, in dumps(x, buffer_callback, protocol)\n     76     buffers.clear()\n---&gt; 77     result = cloudpickle.dumps(x, **dump_kwargs)\n     78 except Exception:\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1537, in dumps(obj, protocol, buffer_callback)\n   1536 cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)\n-&gt; 1537 cp.dump(obj)\n   1538 return file.getvalue()\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1303, in Pickler.dump(self, obj)\n   1302 try:\n-&gt; 1303     return super().dump(obj)\n   1304 except RuntimeError as e:\n\nPicklingError: Can't pickle &lt;built-in function input&gt;: it's not the same object as builtins.input\n\nThe above exception was the direct cause of the following exception:\n\nTypeError                                 Traceback (most recent call last)\nCell In[11], line 4\n      1 fig, ax = plt.subplots(figsize=(8, 6))\n      3 with Profiler() as prof, ResourceProfiler() as rprof, CacheProfiler() as cprof, ProgressBar():\n----&gt; 4     result = normalized_raster.compute()\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataarray.py:1206, in DataArray.compute(self, **kwargs)\n   1181 \"\"\"Manually trigger loading of this array's data from disk or a\n   1182 remote source into memory and return a new array.\n   1183 \n   (...)   1203 dask.compute\n   1204 \"\"\"\n   1205 new = self.copy(deep=False)\n-&gt; 1206 return new.load(**kwargs)\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataarray.py:1174, in DataArray.load(self, **kwargs)\n   1154 def load(self, **kwargs) -&gt; Self:\n   1155     \"\"\"Manually trigger loading of this array's data from disk or a\n   1156     remote source into memory and return this array.\n   1157 \n   (...)   1172     dask.compute\n   1173     \"\"\"\n-&gt; 1174     ds = self._to_temp_dataset().load(**kwargs)\n   1175     new = self._from_temp_dataset(ds)\n   1176     self._variable = new._variable\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataset.py:900, in Dataset.load(self, **kwargs)\n    897 chunkmanager = get_chunked_array_type(*lazy_data.values())\n    899 # evaluate all the chunked arrays simultaneously\n--&gt; 900 evaluated_data: tuple[np.ndarray[Any, Any], ...] = chunkmanager.compute(\n    901     *lazy_data.values(), **kwargs\n    902 )\n    904 for k, data in zip(lazy_data, evaluated_data, strict=False):\n    905     self.variables[k].data = data\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:85, in DaskManager.compute(self, *data, **kwargs)\n     80 def compute(\n     81     self, *data: Any, **kwargs: Any\n     82 ) -&gt; tuple[np.ndarray[Any, _DType_co], ...]:\n     83     from dask.array import compute\n---&gt; 85     return compute(*data, **kwargs)\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\dask\\base.py:662, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    659     postcomputes.append(x.__dask_postcompute__())\n    661 with shorten_traceback():\n--&gt; 662     results = schedule(dsk, keys, **kwargs)\n    664 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:392, in serialize(x, serializers, on_error, context, iterate_collection)\n    390     except Exception:\n    391         raise TypeError(msg) from exc\n--&gt; 392     raise TypeError(msg, str_x) from exc\n    393 else:  # pragma: nocover\n    394     raise ValueError(f\"{on_error=}; expected 'message' or 'raise'\")\n\nTypeError: ('Could not serialize object of type HighLevelGraph', '&lt;ToPickle: HighLevelGraph with 1 layers.\\n&lt;dask.highlevelgraph.HighLevelGraph object at 0x1f3dbc91f90&gt;\\n 0. 2146875929152\\n&gt;')</pre> In\u00a0[\u00a0]: Copied! <pre>ax.imshow(result, cmap=\"gray\")\nax.set_title(\"Znormalizowane pasmo czerwone Sentinel-2\")\nplt.show()\n</pre> ax.imshow(result, cmap=\"gray\") ax.set_title(\"Znormalizowane pasmo czerwone Sentinel-2\") plt.show() In\u00a0[\u00a0]: Copied! <pre>display(prof.visualize())  # Czas wykonania poszczeg\u00f3lnych operacji\ndisplay(rprof.visualize()) # U\u017cycie CPU i pami\u0119ci\n</pre> display(prof.visualize())  # Czas wykonania poszczeg\u00f3lnych operacji display(rprof.visualize()) # U\u017cycie CPU i pami\u0119ci"},{"location":"dask/#dask","title":"Dask\u00b6","text":"<p>Dask to biblioteka, kt\u00f3ra umo\u017cliwia r\u00f3wnoleg\u0142e przetwarzanie du\u017cych zbior\u00f3w danych, rozdzielaj\u0105c je na mniejsze fragmenty i przetwarzaj\u0105c je r\u00f3wnocze\u015bnie. Jest szczeg\u00f3lnie przydatna w analizie danych przestrzennych, gdzie pliki mog\u0105 by\u0107 zbyt du\u017ce, by zmie\u015bci\u0107 si\u0119 w pami\u0119ci RAM.</p> <p>Zalety Daska:</p> <ul> <li>Przetwarzanie r\u00f3wnoleg\u0142e \u2013 dzia\u0142a na wielu rdzeniach CPU lub w klastrze obliczeniowym.</li> <li>Dynamiczne skalowanie \u2013 mo\u017cna pracowa\u0107 zar\u00f3wno na laptopie, jak i w \u015brodowisku rozproszonym.</li> <li>Interoperacyjno\u015b\u0107 \u2013 wsp\u00f3\u0142pracuje z Pandas, Xarray, NumPy, a tak\u017ce narz\u0119dziami GIS jak rasterio czy rioxarray.</li> <li>Lazy evaluation \u2013 nie wykonuje oblicze\u0144 od razu, tylko buduje graf zada\u0144, kt\u00f3ry mo\u017cna uruchomi\u0107 dopiero, gdy jest to potrzebne.</li> </ul>"},{"location":"dask/#rioxarray","title":"Rioxarray\u00b6","text":"<p>Rioxarray to rozszerzenie dla xarray, kt\u00f3re dodaje wsparcie dla georeferencyjnych danych rastrowych. Umo\u017cliwia:</p> <ul> <li>\u0142atwe wczytywanie i zapisywanie plik\u00f3w GeoTIFF,</li> <li>reprojekcj\u0119 i analiz\u0119 przestrzenn\u0105,</li> <li>interakcj\u0119 z innymi narz\u0119dziami GIS (Rasterio, GDAL).</li> </ul> <p>Rioxarray \u015bwietnie integruje si\u0119 z Daskiem, umo\u017cliwiaj\u0105c przetwarzanie ogromnych zbior\u00f3w danych rastrowych w spos\u00f3b efektywny pami\u0119ciowo.</p>"},{"location":"dask/#przeszukiwanie-stac","title":"Przeszukiwanie STAC\u00b6","text":""},{"location":"dask/#inicjalizacja-klienta-dask","title":"Inicjalizacja klienta Dask\u00b6","text":""},{"location":"dask/#pobranie-pliku","title":"Pobranie pliku\u00b6","text":""},{"location":"dask/#podglad-workerow","title":"Podgl\u0105d worker\u00f3w\u00b6","text":""},{"location":"dask/#normalizacja-obrazu","title":"Normalizacja obrazu\u00b6","text":""},{"location":"dask/#uruchomienie-przetwarzania","title":"Uruchomienie przetwarzania\u00b6","text":""},{"location":"dask/#wizualizacja","title":"Wizualizacja\u00b6","text":""},{"location":"dask/#analiza-profilowania","title":"Analiza profilowania\u00b6","text":""},{"location":"geoparquet/","title":"\ud83d\udc7d GeoParquet","text":"In\u00a0[\u00a0]: Copied! <pre>import geopandas as gpd\nimport folium\nfrom shapely.geometry import box\nfrom fsspec.implementations.http import HTTPFileSystem\nimport pyarrow.parquet as pq\nfrom geopandas.io.arrow import _arrow_to_geopandas\nfrom shapely.geometry import box\nfrom folium.plugins import HeatMap\n</pre> import geopandas as gpd import folium from shapely.geometry import box from fsspec.implementations.http import HTTPFileSystem import pyarrow.parquet as pq from geopandas.io.arrow import _arrow_to_geopandas from shapely.geometry import box from folium.plugins import HeatMap In\u00a0[\u00a0]: Copied! <pre>file_url = \"https://data.source.coop/cholmes/eurocrops/unprojected/geoparquet/FR_2018_EC21.parquet\"\nfilesystem = HTTPFileSystem()\n</pre> file_url = \"https://data.source.coop/cholmes/eurocrops/unprojected/geoparquet/FR_2018_EC21.parquet\" filesystem = HTTPFileSystem() <p>To tylko wczytuje metadane</p> In\u00a0[\u00a0]: Copied! <pre>parquet_file = pq.ParquetFile(file_url, filesystem=filesystem)\nprint(f\"Columns: {parquet_file.schema_arrow.names}\")\nprint(f\"Number of rows: {parquet_file.metadata.num_rows}\")\nprint(f\"Number of row groups: {parquet_file.num_row_groups}\")\n</pre> parquet_file = pq.ParquetFile(file_url, filesystem=filesystem) print(f\"Columns: {parquet_file.schema_arrow.names}\") print(f\"Number of rows: {parquet_file.metadata.num_rows}\") print(f\"Number of row groups: {parquet_file.num_row_groups}\") <p>Tutaj wczytujemy faktycznie dane, ale tylko dla jednej z grup wierszy</p> In\u00a0[\u00a0]: Copied! <pre>pyarrow_table = parquet_file.read_row_group(0, columns=[\"ID_PARCEL\", \"SURF_PARC\", \"geometry\"])\ngeopandas_gdf = _arrow_to_geopandas(pyarrow_table)\n</pre> pyarrow_table = parquet_file.read_row_group(0, columns=[\"ID_PARCEL\", \"SURF_PARC\", \"geometry\"]) geopandas_gdf = _arrow_to_geopandas(pyarrow_table) In\u00a0[\u00a0]: Copied! <pre>geopandas_gdf.head(10)\n</pre> geopandas_gdf.head(10) In\u00a0[\u00a0]: Copied! <pre># Przekszta\u0142cenie do EPSG:4326 je\u015bli dane s\u0105 w innym uk\u0142adzie\nif geopandas_gdf.crs.to_epsg() != 4326:\n    geopandas_gdf = geopandas_gdf.to_crs(epsg=4326)\n</pre> # Przekszta\u0142cenie do EPSG:4326 je\u015bli dane s\u0105 w innym uk\u0142adzie if geopandas_gdf.crs.to_epsg() != 4326:     geopandas_gdf = geopandas_gdf.to_crs(epsg=4326) In\u00a0[\u00a0]: Copied! <pre># Usuni\u0119cie pustych i nieprawid\u0142owych geometrii\ngeopandas_gdf = geopandas_gdf[~geopandas_gdf.geometry.is_empty &amp; geopandas_gdf.geometry.notnull()]\n\n# Naprawa geometrii, aby unikn\u0105\u0107 b\u0142\u0119d\u00f3w topologicznych\ngeopandas_gdf['geometry'] = geopandas_gdf['geometry'].buffer(0)\n</pre> # Usuni\u0119cie pustych i nieprawid\u0142owych geometrii geopandas_gdf = geopandas_gdf[~geopandas_gdf.geometry.is_empty &amp; geopandas_gdf.geometry.notnull()]  # Naprawa geometrii, aby unikn\u0105\u0107 b\u0142\u0119d\u00f3w topologicznych geopandas_gdf['geometry'] = geopandas_gdf['geometry'].buffer(0) In\u00a0[\u00a0]: Copied! <pre># Definicja granic Francji w EPSG:4326\nfrance_bbox = box(-5.0, 41.0, 9.7, 51.1)\n\n# Usuni\u0119cie geometrii znajduj\u0105cych si\u0119 poza granicami Francji\ngeopandas_gdf = geopandas_gdf[geopandas_gdf.geometry.within(france_bbox)]\n</pre> # Definicja granic Francji w EPSG:4326 france_bbox = box(-5.0, 41.0, 9.7, 51.1)  # Usuni\u0119cie geometrii znajduj\u0105cych si\u0119 poza granicami Francji geopandas_gdf = geopandas_gdf[geopandas_gdf.geometry.within(france_bbox)] In\u00a0[\u00a0]: Copied! <pre># Konwersja kolumny SURF_PARC do float, je\u015bli zawiera decimal.Decimal\ngeopandas_gdf['SURF_PARC'] = geopandas_gdf['SURF_PARC'].astype(float)\n\n# Obliczenie progu dla 10% najwi\u0119kszych warto\u015bci powierzchni\nthreshold = geopandas_gdf['SURF_PARC'].quantile(0.9)\n</pre> # Konwersja kolumny SURF_PARC do float, je\u015bli zawiera decimal.Decimal geopandas_gdf['SURF_PARC'] = geopandas_gdf['SURF_PARC'].astype(float)  # Obliczenie progu dla 10% najwi\u0119kszych warto\u015bci powierzchni threshold = geopandas_gdf['SURF_PARC'].quantile(0.9) In\u00a0[\u00a0]: Copied! <pre># Filtrowanie 10% najwi\u0119kszych parceli\ngdf_top_10 = geopandas_gdf[geopandas_gdf['SURF_PARC'] &gt;= threshold]\n</pre> # Filtrowanie 10% najwi\u0119kszych parceli gdf_top_10 = geopandas_gdf[geopandas_gdf['SURF_PARC'] &gt;= threshold] In\u00a0[\u00a0]: Copied! <pre># Tworzenie mapy interaktywnej\nm = folium.Map(location=[gdf_top_10.geometry.centroid.y.mean(), gdf_top_10.geometry.centroid.x.mean()], zoom_start=10)\n\n# Dodanie warstwy polygon\u00f3w\ndef add_gdf_to_map(gdf, fmap):\n    for _, row in gdf.iterrows():\n        folium.GeoJson(row.geometry).add_to(fmap)\n\nadd_gdf_to_map(gdf_top_10, m)\n\n# Wy\u015bwietlenie mapy\nm\n</pre> # Tworzenie mapy interaktywnej m = folium.Map(location=[gdf_top_10.geometry.centroid.y.mean(), gdf_top_10.geometry.centroid.x.mean()], zoom_start=10)  # Dodanie warstwy polygon\u00f3w def add_gdf_to_map(gdf, fmap):     for _, row in gdf.iterrows():         folium.GeoJson(row.geometry).add_to(fmap)  add_gdf_to_map(gdf_top_10, m)  # Wy\u015bwietlenie mapy m In\u00a0[\u00a0]: Copied! <pre># Tworzenie mapy heatmap\nheatmap_data = [[point.y, point.x] for point in geopandas_gdf.geometry.centroid]\n\nm_heatmap = folium.Map(location=[gdf_top_10.geometry.centroid.y.mean(), gdf_top_10.geometry.centroid.x.mean()], zoom_start=10)\nHeatMap(heatmap_data).add_to(m_heatmap)\n\n# Wy\u015bwietlenie mapy heatmap\nm_heatmap\n</pre> # Tworzenie mapy heatmap heatmap_data = [[point.y, point.x] for point in geopandas_gdf.geometry.centroid]  m_heatmap = folium.Map(location=[gdf_top_10.geometry.centroid.y.mean(), gdf_top_10.geometry.centroid.x.mean()], zoom_start=10) HeatMap(heatmap_data).add_to(m_heatmap)  # Wy\u015bwietlenie mapy heatmap m_heatmap"},{"location":"geoparquet/#geoparquet","title":"GeoParquet\u00b6","text":"<p>GeoParquet to format zoptymalizowany do przechowywania i przetwarzania danych geoprzestrzennych.</p> <ul> <li>Umo\u017cliwia szybkie odczytywanie i zapisywanie du\u017cych zbior\u00f3w danych dzi\u0119ki kolumnowej strukturze.</li> <li>Obs\u0142uguje r\u00f3wnoleg\u0142e przetwarzanie, co czyni go idealnym do analizy du\u017cych zbior\u00f3w danych.</li> <li>Umo\u017cliwia efektywne filtrowanie i kompresj\u0119, zmniejszaj\u0105c ilo\u015b\u0107 przesy\u0142anych i zapisywanych danych.</li> <li>W przeciwie\u0144stwie do format\u00f3w takich jak Shapefile czy GeoJSON, pozwala na przechowywanie atrybut\u00f3w o r\u00f3\u017cnych typach danych bez ogranicze\u0144.</li> </ul>"},{"location":"geoparquet/#wczytanie-pliku-geoparquet-do-geodataframe-i-podglad-danych","title":"Wczytanie pliku GeoParquet do GeoDataFrame i podgl\u0105d danych\u00b6","text":"<p>GeoPandas to rozszerzenie dla Pandas, kt\u00f3re umo\u017cliwia prac\u0119 z danymi przestrzennymi w Pythonie. U\u0142atwia analiz\u0119 geometrii (punkty, linie, poligony), przekszta\u0142canie uk\u0142ad\u00f3w wsp\u00f3\u0142rz\u0119dnych, operacje przestrzenne (przeci\u0119cia, \u0142\u0105czenia, buforowanie) oraz wizualizacj\u0119 na mapach.</p> <p>GeoPandas wykorzystuje Shapely do obs\u0142ugi geometrii, Fiona do wczytywania danych wektorowych, a Matplotlib do prostych wizualizacji.</p>"},{"location":"geoparquet/#naprawianie-geometrii","title":"Naprawianie geometrii\u00b6","text":""},{"location":"geoparquet/#filtrowanie","title":"Filtrowanie\u00b6","text":""},{"location":"geoparquet/#mapa-interaktywna","title":"Mapa interaktywna\u00b6","text":""},{"location":"geoparquet/#heatmap","title":"Heatmap\u00b6","text":""},{"location":"klify_mapa/","title":"Klify mapa","text":"In\u00a0[41]: Copied! <pre>from osgeo import gdal\nimport pystac_client\nimport planetary_computer as pc\nimport rasterio\nimport numpy as np\nimport folium\nfrom folium.raster_layers import ImageOverlay\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom rasterio.mask import mask\nfrom rasterio.warp import transform_geom, calculate_default_transform, reproject, Resampling\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport io\nimport base64\n</pre> from osgeo import gdal import pystac_client import planetary_computer as pc import rasterio import numpy as np import folium from folium.raster_layers import ImageOverlay import matplotlib.pyplot as plt from PIL import Image from rasterio.mask import mask from rasterio.warp import transform_geom, calculate_default_transform, reproject, Resampling import matplotlib.pyplot as plt import matplotlib.colors as mcolors import io import base64 In\u00a0[42]: Copied! <pre># Po\u0142\u0105czenie z publicznym katalogiem STAC na Azure Planetary Computer\nstac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\nstac_client = pystac_client.Client.open(stac_url)\n</pre> # Po\u0142\u0105czenie z publicznym katalogiem STAC na Azure Planetary Computer stac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\" stac_client = pystac_client.Client.open(stac_url) In\u00a0[43]: Copied! <pre>cords = [53.87, -0.04]\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[\n        [cords[1]-0.1, cords[0]-0.1], [cords[1]+0.05, cords[0]-0.1], [cords[1]+0.05, cords[0]+0.1], [cords[1]-0.1, cords[0]+0.1], [cords[1]-0.1, cords[0]-0.1]\n    ]]\n}\n</pre> cords = [53.87, -0.04] aoi = {     \"type\": \"Polygon\",     \"coordinates\": [[         [cords[1]-0.1, cords[0]-0.1], [cords[1]+0.05, cords[0]-0.1], [cords[1]+0.05, cords[0]+0.1], [cords[1]-0.1, cords[0]+0.1], [cords[1]-0.1, cords[0]-0.1]     ]] } In\u00a0[44]: Copied! <pre># Wyszukiwanie danych Sentinel-1 spe\u0142niaj\u0105cych kryteria\ntime_range = \"2023-06-01/2023-06-30\"\nsearch = stac_client.search(\n    collections=[\"sentinel-1-rtc\"],\n    intersects=aoi,\n    datetime=time_range,\n    max_items=5\n)\n\nitems = list(search.items())\nprint(f\"Znaleziono {len(items)} scen Sentinel-1\")\nprint(f\"Nazwy scen: {items}\")\n</pre> # Wyszukiwanie danych Sentinel-1 spe\u0142niaj\u0105cych kryteria time_range = \"2023-06-01/2023-06-30\" search = stac_client.search(     collections=[\"sentinel-1-rtc\"],     intersects=aoi,     datetime=time_range,     max_items=5 )  items = list(search.items()) print(f\"Znaleziono {len(items)} scen Sentinel-1\") print(f\"Nazwy scen: {items}\") <pre>Znaleziono 5 scen Sentinel-1\nNazwy scen: [&lt;Item id=S1A_IW_GRDH_1SDV_20230629T175034_20230629T175059_049204_05EAAA_rtc&gt;, &lt;Item id=S1A_IW_GRDH_1SDV_20230626T061442_20230626T061507_049153_05E91D_rtc&gt;, &lt;Item id=S1A_IW_GRDH_1SDV_20230624T174231_20230624T174256_049131_05E86A_rtc&gt;, &lt;Item id=S1A_IW_GRDH_1SDV_20230619T062302_20230619T062327_049051_05E600_rtc&gt;, &lt;Item id=S1A_IW_GRDH_1SDV_20230617T175034_20230617T175059_049029_05E55D_rtc&gt;]\n</pre> In\u00a0[45]: Copied! <pre># Wybierz pierwsz\u0105 scen\u0119\nitem = items[0]\n</pre> # Wybierz pierwsz\u0105 scen\u0119 item = items[0] In\u00a0[46]: Copied! <pre># Uzyskaj URL do assetu 'vv'\nhref_vv = pc.sign(item.assets[\"vv\"].href)\nhref_vh = pc.sign(item.assets[\"vh\"].href)\n</pre> # Uzyskaj URL do assetu 'vv' href_vv = pc.sign(item.assets[\"vv\"].href) href_vh = pc.sign(item.assets[\"vh\"].href) In\u00a0[48]: Copied! <pre># Przyci\u0119cie zobrazowania do AOI\ndef clip_raster(dataset, aoi):\n    from shapely.geometry import shape\n    import json\n    \n    # Konwersja AOI do uk\u0142adu wsp\u00f3\u0142rz\u0119dnych rastra\n    aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))\n    \n    aoi_geom = [json.loads(json.dumps(aoi_transformed))]\n    clipped_array, clipped_transform = mask(dataset, [shape(aoi_transformed)], crop=True)\n    return clipped_array[0], clipped_transform\n\ndef reproject_array(array, src_crs, dst_crs, src_transform):\n    dst_transform, width, height = calculate_default_transform(\n        src_crs, dst_crs, array.shape[1], array.shape[0], *rasterio.transform.array_bounds(array.shape[0], array.shape[1], src_transform)\n    )\n    dst_array = np.empty((height, width), dtype=np.float32)\n\n    reproject(\n        source=array,\n        destination=dst_array,\n        src_transform=src_transform,\n        src_crs=src_crs,\n        dst_transform=dst_transform,\n        dst_crs=dst_crs,\n        resampling=Resampling.nearest\n    )\n    return dst_array, dst_transform\n</pre> # Przyci\u0119cie zobrazowania do AOI def clip_raster(dataset, aoi):     from shapely.geometry import shape     import json          # Konwersja AOI do uk\u0142adu wsp\u00f3\u0142rz\u0119dnych rastra     aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))          aoi_geom = [json.loads(json.dumps(aoi_transformed))]     clipped_array, clipped_transform = mask(dataset, [shape(aoi_transformed)], crop=True)     return clipped_array[0], clipped_transform  def reproject_array(array, src_crs, dst_crs, src_transform):     dst_transform, width, height = calculate_default_transform(         src_crs, dst_crs, array.shape[1], array.shape[0], *rasterio.transform.array_bounds(array.shape[0], array.shape[1], src_transform)     )     dst_array = np.empty((height, width), dtype=np.float32)      reproject(         source=array,         destination=dst_array,         src_transform=src_transform,         src_crs=src_crs,         dst_transform=dst_transform,         dst_crs=dst_crs,         resampling=Resampling.nearest     )     return dst_array, dst_transform In\u00a0[49]: Copied! <pre>with rasterio.open(href_vv) as vv_ds:\n    vv, red_transform = clip_raster(vv_ds, aoi)\n    # Zamie\u0144 -32768 na NaN\n    vv = np.where(vv == -32768, np.nan, vv)\n    #vv = vv.read(1)\n    vv_min, vv_max = np.nanmin(vv), np.nanmax(vv)\n    print(f\"VV kana\u0142: min = {vv_min}, max = {vv_max}\")\n\nwith rasterio.open(href_vh) as vh_ds:\n    vh, red_transform = clip_raster(vh_ds, aoi)\n    # Zamie\u0144 -32768 na NaN\n    vh = np.where(vh == -32768, np.nan, vh)\n    #vv = vv.read(1)\n    vh_min, vh_max = np.nanmin(vh), np.nanmax(vh)\n    print(f\"VV kana\u0142: min = {vh_min}, max = {vh_max}\")\n</pre> with rasterio.open(href_vv) as vv_ds:     vv, red_transform = clip_raster(vv_ds, aoi)     # Zamie\u0144 -32768 na NaN     vv = np.where(vv == -32768, np.nan, vv)     #vv = vv.read(1)     vv_min, vv_max = np.nanmin(vv), np.nanmax(vv)     print(f\"VV kana\u0142: min = {vv_min}, max = {vv_max}\")  with rasterio.open(href_vh) as vh_ds:     vh, red_transform = clip_raster(vh_ds, aoi)     # Zamie\u0144 -32768 na NaN     vh = np.where(vh == -32768, np.nan, vh)     #vv = vv.read(1)     vh_min, vh_max = np.nanmin(vh), np.nanmax(vh)     print(f\"VV kana\u0142: min = {vh_min}, max = {vh_max}\") <pre>VV kana\u0142: min = 0.0002965245221275836, max = 29.3183536529541\nVV kana\u0142: min = 0.0001243101869476959, max = 3.0979485511779785\n</pre> In\u00a0[\u00a0]: Copied! <pre>#dodanie zdj\u0119cia SAR do mapy\nwith rasterio.open(href_vh) as vh_ds:\n    vh, red_transform = clip_raster(vh_ds, aoi)\n    # Zamie\u0144 -32768 na NaN\n    vh = np.where(vh == -32768, np.nan, vh)\n    #vv = vv.read(1)\n    #vh_min, vh_max = np.nanmin(vh), np.nanmax(vh)\n    vh = vh.astype(np.float32)\n    #vh, red_transform = reproject_array(vh, vh_ds.crs, 'EPSG:4326', red_transform) #TODO fix reprojection\n    from rasterio.warp import transform_bounds\n    #print(f\"VV kana\u0142: min = {vh_min}, max = {vh_max}\")\n\n#dodanie zdj\u0119cia SAR do mapy\nwith rasterio.open(href_vv) as vv_ds:\n    vv, red_transform = clip_raster(vv_ds, aoi)\n    # Zamie\u0144 -32768 na NaN\n    vv = np.where(vv == -32768, np.nan, vv)\n    #vv = vv.read(1)\n    #vh_min, vh_max = np.nanmin(vh), np.nanmax(vh)\n    vv = vv.astype(np.float32)\n    #vv, red_transform = reproject_array(vv, vv_ds.crs, 'EPSG:4326', red_transform) #TODO fix reprojection\n    from rasterio.warp import transform_bounds\n    #print(f\"VV kana\u0142: min = {vh_min}, max = {vh_max}\")\n</pre> #dodanie zdj\u0119cia SAR do mapy with rasterio.open(href_vh) as vh_ds:     vh, red_transform = clip_raster(vh_ds, aoi)     # Zamie\u0144 -32768 na NaN     vh = np.where(vh == -32768, np.nan, vh)     #vv = vv.read(1)     #vh_min, vh_max = np.nanmin(vh), np.nanmax(vh)     vh = vh.astype(np.float32)     #vh, red_transform = reproject_array(vh, vh_ds.crs, 'EPSG:4326', red_transform) #TODO fix reprojection     from rasterio.warp import transform_bounds     #print(f\"VV kana\u0142: min = {vh_min}, max = {vh_max}\")  #dodanie zdj\u0119cia SAR do mapy with rasterio.open(href_vv) as vv_ds:     vv, red_transform = clip_raster(vv_ds, aoi)     # Zamie\u0144 -32768 na NaN     vv = np.where(vv == -32768, np.nan, vv)     #vv = vv.read(1)     #vh_min, vh_max = np.nanmin(vh), np.nanmax(vh)     vv = vv.astype(np.float32)     #vv, red_transform = reproject_array(vv, vv_ds.crs, 'EPSG:4326', red_transform) #TODO fix reprojection     from rasterio.warp import transform_bounds     #print(f\"VV kana\u0142: min = {vh_min}, max = {vh_max}\") In\u00a0[52]: Copied! <pre># Pobranie uk\u0142adu wsp\u00f3\u0142rz\u0119dnych pliku\ndataset_crs = vh_ds.crs\n\n# Przekszta\u0142cenie granic obrazu do WGS84\nheight, width = vh.shape\n# Obliczenie granic po przyci\u0119ciu\nleft, top = red_transform * (0, 0)  # Lewy g\u00f3rny r\u00f3g\nright, bottom = red_transform * (width, height)  # Prawy dolny r\u00f3g\nbounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top)\n</pre> # Pobranie uk\u0142adu wsp\u00f3\u0142rz\u0119dnych pliku dataset_crs = vh_ds.crs  # Przekszta\u0142cenie granic obrazu do WGS84 height, width = vh.shape # Obliczenie granic po przyci\u0119ciu left, top = red_transform * (0, 0)  # Lewy g\u00f3rny r\u00f3g right, bottom = red_transform * (width, height)  # Prawy dolny r\u00f3g bounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top) In\u00a0[53]: Copied! <pre>vh_min, vh_max = np.nanmin(vh), np.nanmax(vh)\nvv_min, vv_max = np.nanmin(vv), np.nanmax(vv)\nprint(f\"VH kana\u0142: min = {vh_min}, max = {vh_max}\")\nprint(f\"VV kana\u0142: min = {vv_min}, max = {vv_max}\")\n</pre> vh_min, vh_max = np.nanmin(vh), np.nanmax(vh) vv_min, vv_max = np.nanmin(vv), np.nanmax(vv) print(f\"VH kana\u0142: min = {vh_min}, max = {vh_max}\") print(f\"VV kana\u0142: min = {vv_min}, max = {vv_max}\") <pre>VH kana\u0142: min = 0.0001243101869476959, max = 3.0979485511779785\nVV kana\u0142: min = 0.0002965245221275836, max = 29.3183536529541\n</pre> In\u00a0[54]: Copied! <pre># Normalizacja do zakresu 0-255\n# Tworzenie mapy kolor\u00f3w do wizualizacji NDVI\ncmap = plt.get_cmap('RdYlGn')  # Czerwony-\u017c\u00f3\u0142ty-zielony\nnorm_vh = mcolors.Normalize(vmin=vh_min, vmax=vh_max)\nvh_colored = cmap(norm_vh(vh))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa\nvh_colored = (vh_colored * 255).astype(np.uint8)\n\n# VV map\nnorm_vv = mcolors.Normalize(vmin=vv_min, vmax=vv_max)\nvv_colored = cmap(norm_vv(vv))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa\nvv_colored = (vv_colored * 255).astype(np.uint8)\n\n# Tworzenie obrazu VH\nimage = Image.fromarray(vh_colored, mode=\"RGB\")\nimage = image.convert(\"RGBA\")\n\n# Konwersja obrazu na format base64\nimage_buffer = io.BytesIO()\nimage.save(image_buffer, format='PNG')\nimage_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8')\nimport tempfile\n\n# Zapisanie obrazu NDVI do pliku tymczasowego\ntemp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\nimage.save(temp_file.name, format='PNG')\nimage_ur_vh = temp_file.name\n\n# Tworzenie obrazu VV\nimage = Image.fromarray(vv_colored, mode=\"RGB\")\nimage = image.convert(\"RGBA\")\n\n# Konwersja obrazu na format base64\nimage_buffer = io.BytesIO()\nimage.save(image_buffer, format='PNG')\nimage_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8')\nimport tempfile\n\n# Zapisanie obrazu NDVI do pliku tymczasowego\ntemp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\nimage.save(temp_file.name, format='PNG')\nimage_ur_vv = temp_file.name\n</pre> # Normalizacja do zakresu 0-255 # Tworzenie mapy kolor\u00f3w do wizualizacji NDVI cmap = plt.get_cmap('RdYlGn')  # Czerwony-\u017c\u00f3\u0142ty-zielony norm_vh = mcolors.Normalize(vmin=vh_min, vmax=vh_max) vh_colored = cmap(norm_vh(vh))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa vh_colored = (vh_colored * 255).astype(np.uint8)  # VV map norm_vv = mcolors.Normalize(vmin=vv_min, vmax=vv_max) vv_colored = cmap(norm_vv(vv))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa vv_colored = (vv_colored * 255).astype(np.uint8)  # Tworzenie obrazu VH image = Image.fromarray(vh_colored, mode=\"RGB\") image = image.convert(\"RGBA\")  # Konwersja obrazu na format base64 image_buffer = io.BytesIO() image.save(image_buffer, format='PNG') image_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8') import tempfile  # Zapisanie obrazu NDVI do pliku tymczasowego temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False) image.save(temp_file.name, format='PNG') image_ur_vh = temp_file.name  # Tworzenie obrazu VV image = Image.fromarray(vv_colored, mode=\"RGB\") image = image.convert(\"RGBA\")  # Konwersja obrazu na format base64 image_buffer = io.BytesIO() image.save(image_buffer, format='PNG') image_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8') import tempfile  # Zapisanie obrazu NDVI do pliku tymczasowego temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False) image.save(temp_file.name, format='PNG') image_ur_vv = temp_file.name In\u00a0[55]: Copied! <pre># Tworzenie mapy i dodanie NDVI jako warstwy rastrowej\nminx, miny, maxx, maxy = bounds\nm = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=10)\nimage_overlay = ImageOverlay(\n    image=image_ur_vh,\n    bounds=[[miny, minx], [maxy, maxx]],\n    opacity=0.9,\n    name=\"VH Layer\"\n)\nimage_overlay.add_to(m)\n\nimage_overlay = ImageOverlay(\n    image=image_ur_vv,\n    bounds=[[miny, minx], [maxy, maxx]],\n    opacity=0.9,\n    name=\"VV Layer\"\n)\nimage_overlay.add_to(m)\n\n# Dodanie AOI jako poligon\nfolium.Polygon(\n    locations=[(lat, lon) for lon, lat in aoi[\"coordinates\"][0]],\n    color='blue',\n    weight=2,\n    fill=True,\n    fill_opacity=0.2,\n    popup='AOI'\n).add_to(m)\n\n\n# Dodanie opcji sterowania warstwami\nfolium.LayerControl().add_to(m)\n\n# Wy\u015bwietlenie mapy\nm\n</pre> # Tworzenie mapy i dodanie NDVI jako warstwy rastrowej minx, miny, maxx, maxy = bounds m = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=10) image_overlay = ImageOverlay(     image=image_ur_vh,     bounds=[[miny, minx], [maxy, maxx]],     opacity=0.9,     name=\"VH Layer\" ) image_overlay.add_to(m)  image_overlay = ImageOverlay(     image=image_ur_vv,     bounds=[[miny, minx], [maxy, maxx]],     opacity=0.9,     name=\"VV Layer\" ) image_overlay.add_to(m)  # Dodanie AOI jako poligon folium.Polygon(     locations=[(lat, lon) for lon, lat in aoi[\"coordinates\"][0]],     color='blue',     weight=2,     fill=True,     fill_opacity=0.2,     popup='AOI' ).add_to(m)   # Dodanie opcji sterowania warstwami folium.LayerControl().add_to(m)  # Wy\u015bwietlenie mapy m Out[55]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"klify_maski/","title":"Klify maski","text":"In\u00a0[7]: Copied! <pre>from osgeo import gdal\nimport pystac_client\nimport planetary_computer as pc\nimport rasterio\nimport numpy as np\nimport folium\nfrom folium.raster_layers import ImageOverlay\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom rasterio.mask import mask\nfrom rasterio.warp import transform_geom, transform_bounds, calculate_default_transform, reproject, Resampling\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport io\nimport base64\nfrom tqdm import tqdm\nfrom scipy.ndimage import median_filter\nfrom shapely.geometry import shape\nfrom rasterio.transform import from_origin, rowcol, from_bounds\nimport tempfile\nfrom rasterio.features import shapes\nimport geopandas as gpd\n</pre> from osgeo import gdal import pystac_client import planetary_computer as pc import rasterio import numpy as np import folium from folium.raster_layers import ImageOverlay import matplotlib.pyplot as plt from PIL import Image from rasterio.mask import mask from rasterio.warp import transform_geom, transform_bounds, calculate_default_transform, reproject, Resampling import matplotlib.pyplot as plt import matplotlib.colors as mcolors import io import base64 from tqdm import tqdm from scipy.ndimage import median_filter from shapely.geometry import shape from rasterio.transform import from_origin, rowcol, from_bounds import tempfile from rasterio.features import shapes import geopandas as gpd In\u00a0[2]: Copied! <pre># Po\u0142\u0105czenie z publicznym katalogiem STAC na Azure Planetary Computer\nstac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\nstac_client = pystac_client.Client.open(stac_url)\n</pre> # Po\u0142\u0105czenie z publicznym katalogiem STAC na Azure Planetary Computer stac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\" stac_client = pystac_client.Client.open(stac_url) In\u00a0[3]: Copied! <pre>cords = [53.87, -0.04]\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[\n        [cords[1]-0.1, cords[0]-0.1], [cords[1]+0.05, cords[0]-0.1], [cords[1]+0.05, cords[0]+0.1], [cords[1]-0.1, cords[0]+0.1], [cords[1]-0.1, cords[0]-0.1]\n    ]]\n}\n</pre> cords = [53.87, -0.04] aoi = {     \"type\": \"Polygon\",     \"coordinates\": [[         [cords[1]-0.1, cords[0]-0.1], [cords[1]+0.05, cords[0]-0.1], [cords[1]+0.05, cords[0]+0.1], [cords[1]-0.1, cords[0]+0.1], [cords[1]-0.1, cords[0]-0.1]     ]] } In\u00a0[4]: Copied! <pre>aoi_shape = shape(aoi)\nresolution = 0.0001\n\naoi_minx, aoi_miny, aoi_maxx, aoi_maxy = aoi_shape.bounds\n\naoi_width = int((aoi_maxx - aoi_minx) / resolution)\naoi_height = int((aoi_maxy - aoi_miny) / resolution)\n\ntransform = from_origin(aoi_minx, aoi_maxy, resolution, resolution)\n\nmask_data = np.ones((aoi_height, aoi_width), dtype=np.uint8)\n</pre> aoi_shape = shape(aoi) resolution = 0.0001  aoi_minx, aoi_miny, aoi_maxx, aoi_maxy = aoi_shape.bounds  aoi_width = int((aoi_maxx - aoi_minx) / resolution) aoi_height = int((aoi_maxy - aoi_miny) / resolution)  transform = from_origin(aoi_minx, aoi_maxy, resolution, resolution)  mask_data = np.ones((aoi_height, aoi_width), dtype=np.uint8) In\u00a0[5]: Copied! <pre># Przyci\u0119cie zobrazowania do AOI\ndef clip_raster(dataset, aoi):\n    from shapely.geometry import shape\n    import json\n    \n    # Konwersja AOI do uk\u0142adu wsp\u00f3\u0142rz\u0119dnych rastra\n    aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))\n    \n    aoi_geom = [json.loads(json.dumps(aoi_transformed))]\n    clipped_array, clipped_transform = mask(dataset, [shape(aoi_transformed)], crop=True)\n    return clipped_array[0], clipped_transform\n\ndef reproject_array(array, src_crs, dst_crs, src_transform):\n    dst_transform, width, height = calculate_default_transform(\n        src_crs, dst_crs, array.shape[1], array.shape[0], *rasterio.transform.array_bounds(array.shape[0], array.shape[1], src_transform)\n    )\n    dst_array = np.empty((height, width), dtype=np.float32)\n\n    reproject(\n        source=array,\n        destination=dst_array,\n        src_transform=src_transform,\n        src_crs=src_crs,\n        dst_transform=dst_transform,\n        dst_crs=dst_crs,\n        resampling=Resampling.nearest\n    )\n    return dst_array, dst_transform\n</pre> # Przyci\u0119cie zobrazowania do AOI def clip_raster(dataset, aoi):     from shapely.geometry import shape     import json          # Konwersja AOI do uk\u0142adu wsp\u00f3\u0142rz\u0119dnych rastra     aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))          aoi_geom = [json.loads(json.dumps(aoi_transformed))]     clipped_array, clipped_transform = mask(dataset, [shape(aoi_transformed)], crop=True)     return clipped_array[0], clipped_transform  def reproject_array(array, src_crs, dst_crs, src_transform):     dst_transform, width, height = calculate_default_transform(         src_crs, dst_crs, array.shape[1], array.shape[0], *rasterio.transform.array_bounds(array.shape[0], array.shape[1], src_transform)     )     dst_array = np.empty((height, width), dtype=np.float32)      reproject(         source=array,         destination=dst_array,         src_transform=src_transform,         src_crs=src_crs,         dst_transform=dst_transform,         dst_crs=dst_crs,         resampling=Resampling.nearest     )     return dst_array, dst_transform In\u00a0[6]: Copied! <pre>years = [2016, 2021, 2023] #rok 2022 jest jaki\u015b dziwny, nie polecam\nvh_medians_by_year = {}\n\nfor year in years:\n    print(f\"\\n\ud83d\udd0d Przetwarzanie roku {year}\")\n    time_range = f\"{year}-06-01/{year}-08-01\"\n\n    search = stac_client.search(\n        collections=[\"sentinel-1-rtc\"],\n        intersects=aoi,\n        datetime=time_range,\n    )\n    items = list(search.items())\n    print(f\"Znaleziono {len(items)} scen dla roku {year}\")\n\n    vh_stack = []\n\n    for item in tqdm(items):\n        try:\n            href_vh = pc.sign(item.assets[\"vh\"].href)\n\n            with rasterio.open(href_vh) as vh_ds:\n                vh, red_transform = clip_raster(vh_ds, aoi)\n                vh = np.where(vh == -32768, np.nan, vh).astype(np.float32)\n\n            if not vh_stack:\n                vh_stack.append(vh)\n            elif vh.shape == vh_stack[0].shape:\n                vh_stack.append(vh)\n            else:\n                print(f\"Pomini\u0119to scen\u0119 o innym wymiarze: {item.id}\")\n\n        except Exception as e:\n            print(f\"B\u0142\u0105d w scenie {item.id}: {e}\")\n            continue\n\n    if vh_stack:\n        vh_array = np.stack(vh_stack)\n        vh_median = np.nanmedian(vh_array, axis=0)\n        vh_medians_by_year[year] = vh_median\n        print(f\"\u2705 Obliczono median\u0119 dla roku {year}\")\n    else:\n        print(f\"\u26a0\ufe0f Brak poprawnych danych dla roku {year}\")\n</pre> years = [2016, 2021, 2023] #rok 2022 jest jaki\u015b dziwny, nie polecam vh_medians_by_year = {}  for year in years:     print(f\"\\n\ud83d\udd0d Przetwarzanie roku {year}\")     time_range = f\"{year}-06-01/{year}-08-01\"      search = stac_client.search(         collections=[\"sentinel-1-rtc\"],         intersects=aoi,         datetime=time_range,     )     items = list(search.items())     print(f\"Znaleziono {len(items)} scen dla roku {year}\")      vh_stack = []      for item in tqdm(items):         try:             href_vh = pc.sign(item.assets[\"vh\"].href)              with rasterio.open(href_vh) as vh_ds:                 vh, red_transform = clip_raster(vh_ds, aoi)                 vh = np.where(vh == -32768, np.nan, vh).astype(np.float32)              if not vh_stack:                 vh_stack.append(vh)             elif vh.shape == vh_stack[0].shape:                 vh_stack.append(vh)             else:                 print(f\"Pomini\u0119to scen\u0119 o innym wymiarze: {item.id}\")          except Exception as e:             print(f\"B\u0142\u0105d w scenie {item.id}: {e}\")             continue      if vh_stack:         vh_array = np.stack(vh_stack)         vh_median = np.nanmedian(vh_array, axis=0)         vh_medians_by_year[year] = vh_median         print(f\"\u2705 Obliczono median\u0119 dla roku {year}\")     else:         print(f\"\u26a0\ufe0f Brak poprawnych danych dla roku {year}\") <pre>\n\ud83d\udd0d Przetwarzanie roku 2016\nZnaleziono 14 scen dla roku 2016\n</pre> <pre>  0%|          | 0/14 [00:00&lt;?, ?it/s]</pre> <pre>B\u0142\u0105d w scenie S1A_IW_GRDH_1SDH_20160801T061410_20160801T061440_012403_013596_rtc: 'vh'\n</pre> <pre> 21%|\u2588\u2588\u258f       | 3/14 [00:03&lt;00:12,  1.12s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20160723T174955_20160723T175020_012279_01316F_rtc\n</pre> <pre> 36%|\u2588\u2588\u2588\u258c      | 5/14 [00:05&lt;00:10,  1.19s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20160713T062214_20160713T062239_012126_012C80_rtc\n</pre> <pre> 43%|\u2588\u2588\u2588\u2588\u258e     | 6/14 [00:06&lt;00:09,  1.17s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20160711T174942_20160711T175011_012104_012BC9_rtc\n</pre> <pre> 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 9/14 [00:12&lt;00:07,  1.53s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20160701T062214_20160701T062239_011951_0126C6_rtc\n</pre> <pre> 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 10/14 [00:13&lt;00:06,  1.51s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20160629T174953_20160629T175018_011929_012602_rtc\n</pre> <pre> 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 12/14 [00:16&lt;00:03,  1.52s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20160607T062213_20160607T062238_011601_011BB5_rtc\n</pre> <pre> 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:18&lt;00:01,  1.58s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20160605T174952_20160605T175017_011579_011B03_rtc\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:19&lt;00:00,  1.41s/it]\nC:\\Users\\embra\\AppData\\Local\\Temp\\ipykernel_20524\\1886175484.py:39: RuntimeWarning: All-NaN slice encountered\n  vh_median = np.nanmedian(vh_array, axis=0)\n</pre> <pre>\u2705 Obliczono median\u0119 dla roku 2016\n\n\ud83d\udd0d Przetwarzanie roku 2021\nZnaleziono 38 scen dla roku 2021\n</pre> <pre>  5%|\u258c         | 2/38 [00:02&lt;00:45,  1.27s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210729T062157_20210729T062222_028005_03573F_rtc\n</pre> <pre> 11%|\u2588         | 4/38 [00:05&lt;00:43,  1.28s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210727T174935_20210727T175000_027983_0356A8_rtc\n</pre> <pre> 16%|\u2588\u258c        | 6/38 [00:07&lt;00:37,  1.17s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210723T062253_20210723T062318_038901_049714_rtc\n</pre> <pre> 21%|\u2588\u2588        | 8/38 [00:10&lt;00:40,  1.36s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210721T175025_20210721T175050_038879_049676_rtc\n</pre> <pre> 26%|\u2588\u2588\u258b       | 10/38 [00:13&lt;00:38,  1.37s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210717T062156_20210717T062221_027830_035220_rtc\n</pre> <pre> 32%|\u2588\u2588\u2588\u258f      | 12/38 [00:16&lt;00:37,  1.45s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210715T174935_20210715T175000_027808_03517A_rtc\n</pre> <pre> 37%|\u2588\u2588\u2588\u258b      | 14/38 [00:18&lt;00:33,  1.38s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210711T062252_20210711T062317_038726_0491D7_rtc\n</pre> <pre> 42%|\u2588\u2588\u2588\u2588\u258f     | 16/38 [00:22&lt;00:33,  1.54s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210709T175024_20210709T175049_038704_049136_rtc\n</pre> <pre> 47%|\u2588\u2588\u2588\u2588\u258b     | 18/38 [00:24&lt;00:27,  1.39s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210705T062156_20210705T062221_027655_034CEF_rtc\n</pre> <pre> 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 21/38 [00:27&lt;00:20,  1.20s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210629T062251_20210629T062316_038551_048C97_rtc\n</pre> <pre> 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 23/38 [00:30&lt;00:19,  1.28s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210627T175023_20210627T175048_038529_048BF5_rtc\n</pre> <pre> 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 25/38 [00:33&lt;00:16,  1.26s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210623T062155_20210623T062220_027480_0347FD_rtc\n</pre> <pre> 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 27/38 [00:35&lt;00:13,  1.24s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210621T174933_20210621T174958_027458_03476E_rtc\n</pre> <pre> 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 29/38 [00:37&lt;00:10,  1.12s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210617T062250_20210617T062315_038376_048753_rtc\n</pre> <pre> 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 30/38 [00:38&lt;00:09,  1.16s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210615T175023_20210615T175048_038354_0486B8_rtc\n</pre> <pre> 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 32/38 [00:41&lt;00:06,  1.15s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210611T062154_20210611T062219_027305_0342DE_rtc\n</pre> <pre> 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 34/38 [00:43&lt;00:04,  1.12s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1B_IW_GRDH_1SDV_20210609T174933_20210609T174958_027283_03423A_rtc\n</pre> <pre> 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 36/38 [00:45&lt;00:02,  1.10s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210605T062250_20210605T062315_038201_048221_rtc\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 38/38 [00:47&lt;00:00,  1.26s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20210603T175022_20210603T175047_038179_048182_rtc\n</pre> <pre>\nC:\\Users\\embra\\AppData\\Local\\Temp\\ipykernel_20524\\1886175484.py:39: RuntimeWarning: All-NaN slice encountered\n  vh_median = np.nanmedian(vh_array, axis=0)\n</pre> <pre>\u2705 Obliczono median\u0119 dla roku 2021\n\n\ud83d\udd0d Przetwarzanie roku 2023\nZnaleziono 20 scen dla roku 2023\n</pre> <pre> 15%|\u2588\u258c        | 3/20 [00:03&lt;00:17,  1.05s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230725T062304_20230725T062329_049576_05F614_rtc\n</pre> <pre> 20%|\u2588\u2588        | 4/20 [00:04&lt;00:17,  1.11s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230723T175036_20230723T175101_049554_05F56C_rtc\n</pre> <pre> 30%|\u2588\u2588\u2588       | 6/20 [00:06&lt;00:16,  1.15s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230713T062303_20230713T062328_049401_05F0B8_rtc\n</pre> <pre> 35%|\u2588\u2588\u2588\u258c      | 7/20 [00:08&lt;00:15,  1.21s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230711T175035_20230711T175100_049379_05F019_rtc\n</pre> <pre> 50%|\u2588\u2588\u2588\u2588\u2588     | 10/20 [00:11&lt;00:10,  1.08s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230701T062302_20230701T062327_049226_05EB4B_rtc\n</pre> <pre> 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 11/20 [00:12&lt;00:10,  1.15s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230629T175034_20230629T175059_049204_05EAAA_rtc\n</pre> <pre> 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 14/20 [00:15&lt;00:06,  1.12s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230619T062302_20230619T062327_049051_05E600_rtc\n</pre> <pre> 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 15/20 [00:17&lt;00:06,  1.34s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230617T175034_20230617T175059_049029_05E55D_rtc\n</pre> <pre> 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 18/20 [00:21&lt;00:02,  1.17s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230607T062300_20230607T062325_048876_05E0AB_rtc\n</pre> <pre> 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:22&lt;00:01,  1.28s/it]</pre> <pre>Pomini\u0119to scen\u0119 o innym wymiarze: S1A_IW_GRDH_1SDV_20230605T175033_20230605T175058_048854_05E008_rtc\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:23&lt;00:00,  1.20s/it]\nC:\\Users\\embra\\AppData\\Local\\Temp\\ipykernel_20524\\1886175484.py:39: RuntimeWarning: All-NaN slice encountered\n  vh_median = np.nanmedian(vh_array, axis=0)\n</pre> <pre>\u2705 Obliczono median\u0119 dla roku 2023\n</pre> In\u00a0[8]: Copied! <pre>water_mask_filtered_by_year = {}\n\nfor year in years:\n    water_mask = (vh_medians_by_year.get(year) &lt; 0.007)\n    water_mask_filtered = median_filter(water_mask.astype(np.uint8), size=3)\n    water_mask_filtered_by_year[year] = water_mask_filtered\n    print(f\"Zrobiono mask\u0119 wody dla roku {year}\")\n\n# Pobranie uk\u0142adu wsp\u00f3\u0142rz\u0119dnych pliku\ndataset_crs = vh_ds.crs\n# Przekszta\u0142cenie granic obrazu do WGS84\nheight, width = vh.shape\nleft, top = red_transform * (0, 0)  # Lewy g\u00f3rny r\u00f3g\nright, bottom = red_transform * (width, height)  # Prawy dolny r\u00f3g\nbounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top)\n</pre> water_mask_filtered_by_year = {}  for year in years:     water_mask = (vh_medians_by_year.get(year) &lt; 0.007)     water_mask_filtered = median_filter(water_mask.astype(np.uint8), size=3)     water_mask_filtered_by_year[year] = water_mask_filtered     print(f\"Zrobiono mask\u0119 wody dla roku {year}\")  # Pobranie uk\u0142adu wsp\u00f3\u0142rz\u0119dnych pliku dataset_crs = vh_ds.crs # Przekszta\u0142cenie granic obrazu do WGS84 height, width = vh.shape left, top = red_transform * (0, 0)  # Lewy g\u00f3rny r\u00f3g right, bottom = red_transform * (width, height)  # Prawy dolny r\u00f3g bounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top) <pre>Zrobiono mask\u0119 wody dla roku 2016\nZrobiono mask\u0119 wody dla roku 2021\nZrobiono mask\u0119 wody dla roku 2023\n</pre> In\u00a0[10]: Copied! <pre>water_mask_reprojected_by_year = {}\nfor year in years:\n    water_mask_reprojected, _ = reproject_array(water_mask_filtered_by_year.get(year), dataset_crs, 'EPSG:4326', red_transform)\n    water_mask_reprojected_by_year[year] = water_mask_reprojected\n    print(f\"Zmieniono uklad wsp\u00f3\u0142rzednych dla roku: {year}\")\n</pre> water_mask_reprojected_by_year = {} for year in years:     water_mask_reprojected, _ = reproject_array(water_mask_filtered_by_year.get(year), dataset_crs, 'EPSG:4326', red_transform)     water_mask_reprojected_by_year[year] = water_mask_reprojected     print(f\"Zmieniono uklad wsp\u00f3\u0142rzednych dla roku: {year}\") <pre>Zmieniono uklad wsp\u00f3\u0142rzednych dla roku: 2016\nZmieniono uklad wsp\u00f3\u0142rzednych dla roku: 2021\nZmieniono uklad wsp\u00f3\u0142rzednych dla roku: 2023\n</pre> In\u00a0[\u00a0]: Copied! <pre>water_mask_bounds = water_mask_reprojected_by_year[2023]\nminx, miny, maxx, maxy = bounds\nheight, width = water_mask_bounds.shape\n\n# Odtworzenie transformacji\ntransform_water = from_bounds(minx, miny, maxx, maxy, width, height)\nrow_start, col_start = rowcol(transform_water , aoi_minx, aoi_maxy)  # top-left\nrow_stop, col_stop = rowcol(transform_water , aoi_maxx, aoi_miny)     # bottom-right\n\n\nrow_start, row_stop = sorted([row_start, row_stop])\ncol_start, col_stop = sorted([col_start, col_stop])\n\nwater_mask_clipped_by_year = {}\nfor year in years:\n    water_mask_clipped = water_mask_reprojected_by_year.get(year)\n    water_mask_clipped = water_mask_clipped[row_start:row_stop, col_start:col_stop]\n    water_mask_clipped_by_year[year] = water_mask_clipped\n    print(f\"Przyci\u0119to mask\u0119 dla roku: {year}\")\n</pre> water_mask_bounds = water_mask_reprojected_by_year[2023] minx, miny, maxx, maxy = bounds height, width = water_mask_bounds.shape  # Odtworzenie transformacji transform_water = from_bounds(minx, miny, maxx, maxy, width, height) row_start, col_start = rowcol(transform_water , aoi_minx, aoi_maxy)  # top-left row_stop, col_stop = rowcol(transform_water , aoi_maxx, aoi_miny)     # bottom-right   row_start, row_stop = sorted([row_start, row_stop]) col_start, col_stop = sorted([col_start, col_stop])  water_mask_clipped_by_year = {} for year in years:     water_mask_clipped = water_mask_reprojected_by_year.get(year)     water_mask_clipped = water_mask_clipped[row_start:row_stop, col_start:col_stop]     water_mask_clipped_by_year[year] = water_mask_clipped     print(f\"Przyci\u0119to mask\u0119 dla roku: {year}\")  <pre>Przyci\u0119to mask\u0119 dla roku: 2016\nPrzyci\u0119to mask\u0119 dla roku: 2021\nPrzyci\u0119to mask\u0119 dla roku: 2023\n</pre> In\u00a0[14]: Copied! <pre># S\u0142ownik z geometriami dla ka\u017cdego roku\nwater_polygons_by_year = {}\n\nfor year in years:\n    water_mask = water_mask_reprojected_by_year.get(year)\n    if water_mask is not None:\n        clipped_mask = water_mask[row_start:row_stop, col_start:col_stop]\n        \n        # Oblicz transformacj\u0119 dla przyci\u0119tej maski\n        transform_clipped = from_bounds(\n            minx + (col_start / width) * (maxx - minx),\n            miny + ((height - row_stop) / height) * (maxy - miny),\n            minx + (col_stop / width) * (maxx - minx),\n            miny + ((height - row_start) / height) * (maxy - miny),\n            clipped_mask.shape[1], clipped_mask.shape[0]\n        )\n\n        # U\u017cyj rasterio.features.shapes do wyodr\u0119bnienia poligon\u00f3w z maski\n        mask_bool = clipped_mask.astype(np.uint8)\n        polygons = list(shapes(mask_bool, mask=mask_bool == 1, transform=transform_clipped))\n        \n        # Tworzenie GeoDataFrame\n        gdf = gpd.GeoDataFrame(\n            [{'geometry': shape(geom), 'year': year} for geom, val in polygons if val == 1],\n            crs=\"EPSG:4326\"  # &lt;- tutaj podaj sw\u00f3j w\u0142a\u015bciwy uk\u0142ad wsp\u00f3\u0142rz\u0119dnych\n        )\n\n        water_polygons_by_year[year] = gdf\n        print(f\"Utworzono {len(gdf)} poligon\u00f3w dla roku {year}\")\n    else:\n        print(f\"Brak danych dla roku: {year}\")\n</pre> # S\u0142ownik z geometriami dla ka\u017cdego roku water_polygons_by_year = {}  for year in years:     water_mask = water_mask_reprojected_by_year.get(year)     if water_mask is not None:         clipped_mask = water_mask[row_start:row_stop, col_start:col_stop]                  # Oblicz transformacj\u0119 dla przyci\u0119tej maski         transform_clipped = from_bounds(             minx + (col_start / width) * (maxx - minx),             miny + ((height - row_stop) / height) * (maxy - miny),             minx + (col_stop / width) * (maxx - minx),             miny + ((height - row_start) / height) * (maxy - miny),             clipped_mask.shape[1], clipped_mask.shape[0]         )          # U\u017cyj rasterio.features.shapes do wyodr\u0119bnienia poligon\u00f3w z maski         mask_bool = clipped_mask.astype(np.uint8)         polygons = list(shapes(mask_bool, mask=mask_bool == 1, transform=transform_clipped))                  # Tworzenie GeoDataFrame         gdf = gpd.GeoDataFrame(             [{'geometry': shape(geom), 'year': year} for geom, val in polygons if val == 1],             crs=\"EPSG:4326\"  # &lt;- tutaj podaj sw\u00f3j w\u0142a\u015bciwy uk\u0142ad wsp\u00f3\u0142rz\u0119dnych         )          water_polygons_by_year[year] = gdf         print(f\"Utworzono {len(gdf)} poligon\u00f3w dla roku {year}\")     else:         print(f\"Brak danych dla roku: {year}\") <pre>Utworzono 46 poligon\u00f3w dla roku 2016\nUtworzono 9 poligon\u00f3w dla roku 2021\nUtworzono 9 poligon\u00f3w dla roku 2023\n</pre> In\u00a0[15]: Copied! <pre>water_mask_colored_by_year = {}\nfor year in years:\n    cmap = plt.get_cmap('RdYlGn')  # Czerwony-\u017c\u00f3\u0142ty-zielony\n    norm = mcolors.Normalize(vmin=0, vmax=1)\n    mask_colored = cmap(norm(water_mask_clipped_by_year.get(year)))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa\n    mask_colored = (mask_colored * 255).astype(np.uint8)\n    # Tworzenie obrazu VH\n    image = Image.fromarray(mask_colored, mode=\"RGB\")\n    image = image.convert(\"RGBA\")\n\n    # Konwersja obrazu na format base64\n    image_buffer = io.BytesIO()\n    image.save(image_buffer, format='PNG')\n    image_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8')\n\n    # Zapisanie obrazu NDVI do pliku tymczasowego\n    temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n    image.save(temp_file.name, format='PNG')\n    image_colored = temp_file.name\n    water_mask_colored_by_year[year] = image_colored\n    print(f\"Dodano warstwe dla roku: {year}\")\n</pre> water_mask_colored_by_year = {} for year in years:     cmap = plt.get_cmap('RdYlGn')  # Czerwony-\u017c\u00f3\u0142ty-zielony     norm = mcolors.Normalize(vmin=0, vmax=1)     mask_colored = cmap(norm(water_mask_clipped_by_year.get(year)))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa     mask_colored = (mask_colored * 255).astype(np.uint8)     # Tworzenie obrazu VH     image = Image.fromarray(mask_colored, mode=\"RGB\")     image = image.convert(\"RGBA\")      # Konwersja obrazu na format base64     image_buffer = io.BytesIO()     image.save(image_buffer, format='PNG')     image_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8')      # Zapisanie obrazu NDVI do pliku tymczasowego     temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)     image.save(temp_file.name, format='PNG')     image_colored = temp_file.name     water_mask_colored_by_year[year] = image_colored     print(f\"Dodano warstwe dla roku: {year}\")  <pre>Dodano warstwe dla roku: 2016\nDodano warstwe dla roku: 2021\nDodano warstwe dla roku: 2023\n</pre> In\u00a0[16]: Copied! <pre># Tworzenie mapy i dodanie NDVI jako warstwy rastrowej\nminx, miny, maxx, maxy = bounds\nm = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=10)\nfor year in years:\n    image_overlay = ImageOverlay(\n        image=water_mask_colored_by_year.get(year),\n        bounds=[[aoi_miny, aoi_minx], [aoi_maxy, aoi_maxx]],\n        opacity=0.5,\n        name=f\"{year} Mask\"\n    )\n    image_overlay.add_to(m)\n\n\nfor year, gdf in water_polygons_by_year.items():\n    geojson = folium.GeoJson(\n        data=gdf.__geo_interface__,\n        name=f\"{year} Poligony\",\n        style_function=lambda feature: {\n            'fillColor': 'blue',\n            'color': 'blue',\n            'weight': 1,\n            'fillOpacity': 0.3,\n        },\n        tooltip=folium.GeoJsonTooltip(fields=['year'])\n    )\n    geojson.add_to(m)\n\n# Dodanie AOI jako poligon\n#folium.Polygon(\n#    locations=[(lat, lon) for lon, lat in aoi[\"coordinates\"][0]],\n#    color='blue',\n#    weight=2,\n#    fill=True,\n#    fill_opacity=0.2,\n#    popup='AOI'\n#).add_to(m)\n\n# Dodanie opcji sterowania warstwami\nfolium.LayerControl().add_to(m)\n\n# Wy\u015bwietlenie mapy\nm\n</pre> # Tworzenie mapy i dodanie NDVI jako warstwy rastrowej minx, miny, maxx, maxy = bounds m = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=10) for year in years:     image_overlay = ImageOverlay(         image=water_mask_colored_by_year.get(year),         bounds=[[aoi_miny, aoi_minx], [aoi_maxy, aoi_maxx]],         opacity=0.5,         name=f\"{year} Mask\"     )     image_overlay.add_to(m)   for year, gdf in water_polygons_by_year.items():     geojson = folium.GeoJson(         data=gdf.__geo_interface__,         name=f\"{year} Poligony\",         style_function=lambda feature: {             'fillColor': 'blue',             'color': 'blue',             'weight': 1,             'fillOpacity': 0.3,         },         tooltip=folium.GeoJsonTooltip(fields=['year'])     )     geojson.add_to(m)  # Dodanie AOI jako poligon #folium.Polygon( #    locations=[(lat, lon) for lon, lat in aoi[\"coordinates\"][0]], #    color='blue', #    weight=2, #    fill=True, #    fill_opacity=0.2, #    popup='AOI' #).add_to(m)  # Dodanie opcji sterowania warstwami folium.LayerControl().add_to(m)  # Wy\u015bwietlenie mapy m Out[16]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"klify_maski/","title":"Klify maski","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport pyproj\n</pre> import os import pyproj In\u00a0[\u00a0]: Copied! <pre>os.environ[\"PROJ_DATA\"] = pyproj.datadir.get_data_dir()\n</pre> os.environ[\"PROJ_DATA\"] = pyproj.datadir.get_data_dir() In\u00a0[\u00a0]: Copied! <pre>import streamlit as st\nimport pystac_client\nimport planetary_computer as pc\nimport rasterio\nimport numpy as np\nimport folium\nfrom folium.raster_layers import ImageOverlay\nfrom PIL import Image\nfrom rasterio.mask import mask\nfrom rasterio.warp import transform_geom, transform_bounds, calculate_default_transform, reproject, Resampling\nfrom scipy.ndimage import median_filter\nfrom shapely.geometry import shape\nfrom rasterio.transform import rowcol, from_bounds\nimport tempfile\nimport json\nimport math\nfrom affine import Affine\nfrom pyproj import Transformer, Geod\nimport zipfile\nfrom pyproj import Geod\nfrom folium.plugins import Draw\nfrom streamlit_folium import st_folium\nfrom folium.plugins import HeatMap\nimport plotly.graph_objects as go\n</pre> import streamlit as st import pystac_client import planetary_computer as pc import rasterio import numpy as np import folium from folium.raster_layers import ImageOverlay from PIL import Image from rasterio.mask import mask from rasterio.warp import transform_geom, transform_bounds, calculate_default_transform, reproject, Resampling from scipy.ndimage import median_filter from shapely.geometry import shape from rasterio.transform import rowcol, from_bounds import tempfile import json import math from affine import Affine from pyproj import Transformer, Geod import zipfile from pyproj import Geod from folium.plugins import Draw from streamlit_folium import st_folium from folium.plugins import HeatMap import plotly.graph_objects as go In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>st.set_page_config(page_title='Abrazj\u0105tka', page_icon='\ud83c\udf0a', layout='wide')\n</pre> st.set_page_config(page_title='Abrazj\u0105tka', page_icon='\ud83c\udf0a', layout='wide') In\u00a0[\u00a0]: Copied! <pre>@st.cache_resource\ndef get_stac_client():\n    stac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n    return pystac_client.Client.open(stac_url)\n</pre> @st.cache_resource def get_stac_client():     stac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"     return pystac_client.Client.open(stac_url) In\u00a0[\u00a0]: Copied! <pre>stac_client = get_stac_client()\n</pre> stac_client = get_stac_client() In\u00a0[\u00a0]: Copied! <pre>st.markdown(\n    \"\"\"\n    &lt;style&gt;\n    [data-testid=\"stSidebar\"] {\n        min-width: 400px;\n        max-width: 400px;\n    }\n    .header-container {\n            display: flex;\n            justify-content: space-between;\n            align-items: center;\n        }\n        .app-title {\n            font-size: 2.5rem;\n            font-weight: bold;\n            margin-left: 1rem;\n        }\n         [data-testid=\"stSidebar\"] {\n        padding-top: 0rem;\n    }\n    &lt;/style&gt;\n    &lt;div class=\"header-container\"&gt;\n        &lt;div class=\"app-title\"&gt;Detekcja zmian powierzchni l\u0105dowej z Sentinel-1&lt;/div&gt;\n    &lt;/div&gt;\n    \"\"\",\n    unsafe_allow_html=True\n)\n</pre> st.markdown(     \"\"\"      Detekcja zmian powierzchni l\u0105dowej z Sentinel-1      \"\"\",     unsafe_allow_html=True ) In\u00a0[\u00a0]: Copied! <pre># Funkcje pomocnicze\ndef clip_raster(dataset, aoi):\n    aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))\n    aoi_geom = [json.loads(json.dumps(aoi_transformed))]\n    clipped_array, clipped_transform = mask(dataset, aoi_geom, crop=True)\n    return clipped_array[0], clipped_transform\n</pre> # Funkcje pomocnicze def clip_raster(dataset, aoi):     aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))     aoi_geom = [json.loads(json.dumps(aoi_transformed))]     clipped_array, clipped_transform = mask(dataset, aoi_geom, crop=True)     return clipped_array[0], clipped_transform In\u00a0[\u00a0]: Copied! <pre>def reproject_array(array, src_crs, dst_crs, src_transform):\n    dst_transform, width, height = calculate_default_transform(\n        src_crs, dst_crs, array.shape[1], array.shape[0], *rasterio.transform.array_bounds(array.shape[0], array.shape[1], src_transform)\n    )\n    dst_array = np.empty((height, width), dtype=np.float32)\n\n    reproject(\n        source=array,\n        destination=dst_array,\n        src_transform=src_transform,\n        src_crs=src_crs,\n        dst_transform=dst_transform,\n        dst_crs=dst_crs,\n        resampling=Resampling.nearest\n    )\n    return dst_array, dst_transform\n</pre> def reproject_array(array, src_crs, dst_crs, src_transform):     dst_transform, width, height = calculate_default_transform(         src_crs, dst_crs, array.shape[1], array.shape[0], *rasterio.transform.array_bounds(array.shape[0], array.shape[1], src_transform)     )     dst_array = np.empty((height, width), dtype=np.float32)      reproject(         source=array,         destination=dst_array,         src_transform=src_transform,         src_crs=src_crs,         dst_transform=dst_transform,         dst_crs=dst_crs,         resampling=Resampling.nearest     )     return dst_array, dst_transform In\u00a0[\u00a0]: Copied! <pre>@st.cache_data(show_spinner=True)\ndef process_data_and_differences(cords, years):\n    lat_center = cords[0]\n    lon_center = cords[1]\n\n    half_side_km = 10\n\n    lat_deg_per_km = 1 / 111\n    lon_deg_per_km = 1 / (111 * math.cos(math.radians(lat_center)))\n\n    delta_lat = half_side_km * lat_deg_per_km\n    delta_lon = half_side_km * lon_deg_per_km\n\n    aoi = {\n        \"type\": \"Polygon\",\n        \"coordinates\": [[\n            [lon_center - delta_lon, lat_center - delta_lat],\n            [lon_center + delta_lon, lat_center - delta_lat],\n            [lon_center + delta_lon, lat_center + delta_lat],\n            [lon_center - delta_lon, lat_center + delta_lat],\n            [lon_center - delta_lon, lat_center - delta_lat]\n        ]]\n    }\n\n    aoi_shape = shape(aoi)\n    resolution = 0.0001\n    aoi_minx, aoi_miny, aoi_maxx, aoi_maxy = aoi_shape.bounds\n\n    vh_medians_by_year = {}\n    vh_ds_crs = None\n    red_transform_global = None\n    transform_treshold = Affine(10.0, 0.0, 289640.00,\n                   0.0, -10.0, 5983810.00)\n\n    # Sort years to ensure consistent comparison order\n    years_sorted = sorted(years)\n\n    for year in years_sorted:\n        time_range = f\"{year}-06-01/{year}-08-01\"\n\n        search = stac_client.search(\n            collections=[\"sentinel-1-rtc\"],\n            intersects=aoi,\n            datetime=time_range,\n        )\n        items = list(search.items())\n        # st.write(f\"Znaleziono {len(items)} scen dla roku {year}\")\n\n        vh_stack = []\n\n        progress_text = f\"Pobieranie i przetwarzanie scen dla roku {year}. Prosz\u0119 czeka\u0107.\"\n        my_bar = st.progress(0, text=progress_text)\n\n        for i, item in enumerate(items):\n            try:\n                href_vh = pc.sign(item.assets[\"vh\"].href)\n\n                with rasterio.open(href_vh) as vh_ds:\n                    if vh_ds_crs is None:\n                        vh_ds_crs = vh_ds.crs\n                    vh, red_transform = clip_raster(vh_ds, aoi)\n                    vh = np.where(vh == -32768, np.nan, vh).astype(np.float32)\n\n                    x1 = red_transform.c\n\n                    x2 = transform_treshold.c\n\n                result = str(int(x1))[0] == str(int(x2))[0] == '2'\n\n                if not vh_stack and result:\n                    vh_stack.append(vh)\n                    red_transform_global = red_transform\n                elif vh.shape == vh_stack[0].shape and result:\n                    vh_stack.append(vh)\n                    red_transform_global = red_transform\n                # else:\n                #     st.warning(f\"Pomini\u0119to scen\u0119 o innym wymiarze lub transform: {item.id}\")\n\n            except Exception as e:\n                # st.error(f\"B\u0142\u0105d w scenie {item.id}: {e}\")\n                continue\n            finally:\n                my_bar.progress((i + 1) / len(items), text=progress_text)\n        my_bar.empty()\n\n        if vh_stack:\n            vh_array = np.stack(vh_stack)\n            vh_median = np.nanmedian(vh_array, axis=0)\n            vh_medians_by_year[year] = vh_median\n            # st.success(f\"\u2705 Obliczono median\u0119 dla roku {year}\")\n        # else:\n            # st.warning(f\"\u26a0\ufe0f Brak poprawnych danych dla roku {year}\")\n\n    if vh_ds_crs is None or red_transform_global is None:\n        st.error(\"Nie uda\u0142o si\u0119 przetworzy\u0107 \u017cadnych danych. Sprawd\u017a obszar zainteresowania i lata.\")\n        return None, None, None, None, None, None, None\n\n    water_mask_filtered_by_year = {}\n    for year in years_sorted:\n        if year in vh_medians_by_year:\n            water_mask = (vh_medians_by_year.get(year) &lt; 0.008)\n            water_mask_filtered = median_filter(water_mask.astype(np.uint8), size=6)\n            water_mask_filtered_by_year[year] = water_mask_filtered\n            # st.write(f\"Zrobiono mask\u0119 wody dla roku {year}\")\n        # else:\n            # st.warning(f\"Brak danych VH dla roku {year}, pomijam generowanie maski wody.\")\n\n    dataset_crs = vh_ds.crs\n    height, width = vh.shape\n    left, top = red_transform_global * (0, 0)\n    right, bottom = red_transform_global * (width, height)\n    bounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top)\n\n    water_mask_reprojected_by_year = {}\n    for year in years_sorted:\n        if year in water_mask_filtered_by_year:\n            water_mask_reprojected, _ = reproject_array(water_mask_filtered_by_year.get(year), dataset_crs, 'EPSG:4326', red_transform_global)\n            water_mask_reprojected_by_year[year] = water_mask_reprojected\n            # st.write(f\"Zmieniono uklad wspulzednych dla roku: {year}\")\n\n    if water_mask_reprojected_by_year:\n        last_year = sorted(water_mask_reprojected_by_year.keys())[-1]\n        sample_mask = water_mask_reprojected_by_year[last_year]\n        minx, miny, maxx, maxy = bounds\n        height, width = sample_mask.shape\n    else:\n        # st.error(\"Brak reprojekcji masek wodnych do okre\u015blenia transformacji.\")\n        return None, None, None, None, None, None, None\n\n    transform_water = from_bounds(minx, miny, maxx, maxy, width, height)\n    row_start, col_start = rowcol(transform_water , aoi_minx, aoi_maxy)\n    row_stop, col_stop = rowcol(transform_water , aoi_maxx, aoi_miny)\n\n    row_start, row_stop = sorted([row_start, row_stop])\n    col_start, col_stop = sorted([col_start, col_stop])\n\n    water_mask_clipped_by_year = {}\n    for year in years_sorted:\n        if year in water_mask_reprojected_by_year:\n            water_mask_clipped = water_mask_reprojected_by_year.get(year)\n            row_start_clip = max(0, row_start)\n            row_stop_clip = min(water_mask_clipped.shape[0], row_stop)\n            col_start_clip = max(0, col_start)\n            col_stop_clip = min(water_mask_clipped.shape[1], col_stop)\n\n            if row_stop_clip &gt; row_start_clip and col_stop_clip &gt; col_start_clip:\n                water_mask_clipped = water_mask_clipped[row_start_clip:row_stop_clip, col_start_clip:col_stop_clip]\n                water_mask_clipped_by_year[year] = water_mask_clipped\n                # st.write(f\"Przyci\u0105to mask\u0119 dla roku: {year}\")\n            else:\n                # st.warning(f\"Przyci\u0119ta maska dla roku {year} jest pusta lub ma niew\u0142a\u015bciwe wymiary.\")\n                water_mask_clipped_by_year[year] = np.zeros((1,1)) \n        # else:\n        #     st.warning(f\"Brak reprojekcji maski wody dla roku {year}, pomijam przycinanie.\")\n\n    water_mask_differences = {}\n    if len(years_sorted) &gt; 1:\n        for i in range(len(years_sorted) - 1):\n            year1 = years_sorted[i]\n            year2 = years_sorted[i+1]\n\n            if year1 in water_mask_clipped_by_year and year2 in water_mask_clipped_by_year:\n                mask1 = water_mask_clipped_by_year[year1]\n                mask2 = water_mask_clipped_by_year[year2]\n\n                if mask1.shape != mask2.shape:\n                    # st.warning(f\"Maski dla lat {year1} i {year2} maj\u0105 r\u00f3\u017cne wymiary. Nie mo\u017cna obliczy\u0107 r\u00f3\u017cnicy.\")\n                    continue\n\n                # Calculate difference:\n                # 1: New water (was land in year1, is water in year2)\n                # -1: Lost water (was water in year1, is land in year2)\n                # 0: No change (land-&gt;land or water-&gt;water)\n                difference_mask = np.zeros_like(mask1, dtype=np.int8)\n                difference_mask[(mask1 == 0) &amp; (mask2 == 1)] = 1  # New water\n                difference_mask[(mask1 == 1) &amp; (mask2 == 0)] = -1 # Lost water\n                water_mask_differences[f\"{year1}-{year2}\"] = difference_mask\n            #     st.success(f\"\u2705 Obliczono r\u00f3\u017cnic\u0119 mi\u0119dzy {year1} a {year2}\")\n            # else:\n            #     st.warning(f\"Brak przyci\u0119tych masek dla lat {year1} lub {year2}. Pomijam obliczanie r\u00f3\u017cnicy.\")\n\n\n    return water_mask_clipped_by_year, water_mask_differences, bounds, aoi_minx, aoi_miny, aoi_maxx, aoi_maxy, red_transform_global\n</pre> @st.cache_data(show_spinner=True) def process_data_and_differences(cords, years):     lat_center = cords[0]     lon_center = cords[1]      half_side_km = 10      lat_deg_per_km = 1 / 111     lon_deg_per_km = 1 / (111 * math.cos(math.radians(lat_center)))      delta_lat = half_side_km * lat_deg_per_km     delta_lon = half_side_km * lon_deg_per_km      aoi = {         \"type\": \"Polygon\",         \"coordinates\": [[             [lon_center - delta_lon, lat_center - delta_lat],             [lon_center + delta_lon, lat_center - delta_lat],             [lon_center + delta_lon, lat_center + delta_lat],             [lon_center - delta_lon, lat_center + delta_lat],             [lon_center - delta_lon, lat_center - delta_lat]         ]]     }      aoi_shape = shape(aoi)     resolution = 0.0001     aoi_minx, aoi_miny, aoi_maxx, aoi_maxy = aoi_shape.bounds      vh_medians_by_year = {}     vh_ds_crs = None     red_transform_global = None     transform_treshold = Affine(10.0, 0.0, 289640.00,                    0.0, -10.0, 5983810.00)      # Sort years to ensure consistent comparison order     years_sorted = sorted(years)      for year in years_sorted:         time_range = f\"{year}-06-01/{year}-08-01\"          search = stac_client.search(             collections=[\"sentinel-1-rtc\"],             intersects=aoi,             datetime=time_range,         )         items = list(search.items())         # st.write(f\"Znaleziono {len(items)} scen dla roku {year}\")          vh_stack = []          progress_text = f\"Pobieranie i przetwarzanie scen dla roku {year}. Prosz\u0119 czeka\u0107.\"         my_bar = st.progress(0, text=progress_text)          for i, item in enumerate(items):             try:                 href_vh = pc.sign(item.assets[\"vh\"].href)                  with rasterio.open(href_vh) as vh_ds:                     if vh_ds_crs is None:                         vh_ds_crs = vh_ds.crs                     vh, red_transform = clip_raster(vh_ds, aoi)                     vh = np.where(vh == -32768, np.nan, vh).astype(np.float32)                      x1 = red_transform.c                      x2 = transform_treshold.c                  result = str(int(x1))[0] == str(int(x2))[0] == '2'                  if not vh_stack and result:                     vh_stack.append(vh)                     red_transform_global = red_transform                 elif vh.shape == vh_stack[0].shape and result:                     vh_stack.append(vh)                     red_transform_global = red_transform                 # else:                 #     st.warning(f\"Pomini\u0119to scen\u0119 o innym wymiarze lub transform: {item.id}\")              except Exception as e:                 # st.error(f\"B\u0142\u0105d w scenie {item.id}: {e}\")                 continue             finally:                 my_bar.progress((i + 1) / len(items), text=progress_text)         my_bar.empty()          if vh_stack:             vh_array = np.stack(vh_stack)             vh_median = np.nanmedian(vh_array, axis=0)             vh_medians_by_year[year] = vh_median             # st.success(f\"\u2705 Obliczono median\u0119 dla roku {year}\")         # else:             # st.warning(f\"\u26a0\ufe0f Brak poprawnych danych dla roku {year}\")      if vh_ds_crs is None or red_transform_global is None:         st.error(\"Nie uda\u0142o si\u0119 przetworzy\u0107 \u017cadnych danych. Sprawd\u017a obszar zainteresowania i lata.\")         return None, None, None, None, None, None, None      water_mask_filtered_by_year = {}     for year in years_sorted:         if year in vh_medians_by_year:             water_mask = (vh_medians_by_year.get(year) &lt; 0.008)             water_mask_filtered = median_filter(water_mask.astype(np.uint8), size=6)             water_mask_filtered_by_year[year] = water_mask_filtered             # st.write(f\"Zrobiono mask\u0119 wody dla roku {year}\")         # else:             # st.warning(f\"Brak danych VH dla roku {year}, pomijam generowanie maski wody.\")      dataset_crs = vh_ds.crs     height, width = vh.shape     left, top = red_transform_global * (0, 0)     right, bottom = red_transform_global * (width, height)     bounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top)      water_mask_reprojected_by_year = {}     for year in years_sorted:         if year in water_mask_filtered_by_year:             water_mask_reprojected, _ = reproject_array(water_mask_filtered_by_year.get(year), dataset_crs, 'EPSG:4326', red_transform_global)             water_mask_reprojected_by_year[year] = water_mask_reprojected             # st.write(f\"Zmieniono uklad wspulzednych dla roku: {year}\")      if water_mask_reprojected_by_year:         last_year = sorted(water_mask_reprojected_by_year.keys())[-1]         sample_mask = water_mask_reprojected_by_year[last_year]         minx, miny, maxx, maxy = bounds         height, width = sample_mask.shape     else:         # st.error(\"Brak reprojekcji masek wodnych do okre\u015blenia transformacji.\")         return None, None, None, None, None, None, None      transform_water = from_bounds(minx, miny, maxx, maxy, width, height)     row_start, col_start = rowcol(transform_water , aoi_minx, aoi_maxy)     row_stop, col_stop = rowcol(transform_water , aoi_maxx, aoi_miny)      row_start, row_stop = sorted([row_start, row_stop])     col_start, col_stop = sorted([col_start, col_stop])      water_mask_clipped_by_year = {}     for year in years_sorted:         if year in water_mask_reprojected_by_year:             water_mask_clipped = water_mask_reprojected_by_year.get(year)             row_start_clip = max(0, row_start)             row_stop_clip = min(water_mask_clipped.shape[0], row_stop)             col_start_clip = max(0, col_start)             col_stop_clip = min(water_mask_clipped.shape[1], col_stop)              if row_stop_clip &gt; row_start_clip and col_stop_clip &gt; col_start_clip:                 water_mask_clipped = water_mask_clipped[row_start_clip:row_stop_clip, col_start_clip:col_stop_clip]                 water_mask_clipped_by_year[year] = water_mask_clipped                 # st.write(f\"Przyci\u0105to mask\u0119 dla roku: {year}\")             else:                 # st.warning(f\"Przyci\u0119ta maska dla roku {year} jest pusta lub ma niew\u0142a\u015bciwe wymiary.\")                 water_mask_clipped_by_year[year] = np.zeros((1,1))          # else:         #     st.warning(f\"Brak reprojekcji maski wody dla roku {year}, pomijam przycinanie.\")      water_mask_differences = {}     if len(years_sorted) &gt; 1:         for i in range(len(years_sorted) - 1):             year1 = years_sorted[i]             year2 = years_sorted[i+1]              if year1 in water_mask_clipped_by_year and year2 in water_mask_clipped_by_year:                 mask1 = water_mask_clipped_by_year[year1]                 mask2 = water_mask_clipped_by_year[year2]                  if mask1.shape != mask2.shape:                     # st.warning(f\"Maski dla lat {year1} i {year2} maj\u0105 r\u00f3\u017cne wymiary. Nie mo\u017cna obliczy\u0107 r\u00f3\u017cnicy.\")                     continue                  # Calculate difference:                 # 1: New water (was land in year1, is water in year2)                 # -1: Lost water (was water in year1, is land in year2)                 # 0: No change (land-&gt;land or water-&gt;water)                 difference_mask = np.zeros_like(mask1, dtype=np.int8)                 difference_mask[(mask1 == 0) &amp; (mask2 == 1)] = 1  # New water                 difference_mask[(mask1 == 1) &amp; (mask2 == 0)] = -1 # Lost water                 water_mask_differences[f\"{year1}-{year2}\"] = difference_mask             #     st.success(f\"\u2705 Obliczono r\u00f3\u017cnic\u0119 mi\u0119dzy {year1} a {year2}\")             # else:             #     st.warning(f\"Brak przyci\u0119tych masek dla lat {year1} lub {year2}. Pomijam obliczanie r\u00f3\u017cnicy.\")       return water_mask_clipped_by_year, water_mask_differences, bounds, aoi_minx, aoi_miny, aoi_maxx, aoi_maxy, red_transform_global In\u00a0[\u00a0]: Copied! <pre>@st.cache_data\ndef generate_difference_image_overlays(water_mask_differences):\n    image_colored_paths_by_diff = {}\n    for diff_label, diff_mask in water_mask_differences.items():\n\n        colored_diff_array = np.zeros((*diff_mask.shape, 4), dtype=np.uint8) \n\n        # Lost water (-1) -&gt; Red\n        colored_diff_array[diff_mask == -1] = [255, 0, 0, 200]\n\n        # New water (1) -&gt; Blue\n        colored_diff_array[diff_mask == 1] = [0, 0, 255, 200] \n\n        # No change (0) -&gt; Transparent\n        colored_diff_array[diff_mask == 0] = [0, 0, 0, 0] \n\n        image = Image.fromarray(colored_diff_array, mode=\"RGBA\")\n\n        temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n        image.save(temp_file.name, format='PNG')\n        image_colored_paths_by_diff[diff_label] = temp_file.name\n\n    return image_colored_paths_by_diff\n</pre> @st.cache_data def generate_difference_image_overlays(water_mask_differences):     image_colored_paths_by_diff = {}     for diff_label, diff_mask in water_mask_differences.items():          colored_diff_array = np.zeros((*diff_mask.shape, 4), dtype=np.uint8)           # Lost water (-1) -&gt; Red         colored_diff_array[diff_mask == -1] = [255, 0, 0, 200]          # New water (1) -&gt; Blue         colored_diff_array[diff_mask == 1] = [0, 0, 255, 200]           # No change (0) -&gt; Transparent         colored_diff_array[diff_mask == 0] = [0, 0, 0, 0]           image = Image.fromarray(colored_diff_array, mode=\"RGBA\")          temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)         image.save(temp_file.name, format='PNG')         image_colored_paths_by_diff[diff_label] = temp_file.name      return image_colored_paths_by_diff In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def compute_land_area_changes(diff_masks, transform, source_crs=\"EPSG:32616\"):\n    geod = Geod(ellps=\"WGS84\")\n    transformer = Transformer.from_crs(source_crs, \"EPSG:4326\", always_xy=True)\n    \n    def process_single_mask(diff_mask):\n        results = {}\n        for value, label in [(-1, \"lost_water\"), (1, \"new_water\")]:\n            rows, cols = np.where(diff_mask == value)\n            total_area = 0.0\n\n            for row, col in zip(rows, cols):\n                x_left, y_top = transform * (col, row)\n                x_right, y_bottom = transform * (col + 1, row + 1)\n\n                lon_left, lat_top = transformer.transform(x_left, y_top)\n                lon_right, lat_bottom = transformer.transform(x_right, y_bottom)\n\n                lons = [lon_left, lon_right, lon_right, lon_left]\n                lats = [lat_top, lat_top, lat_bottom, lat_bottom]\n\n                if any(np.isnan(lons)) or any(np.isnan(lats)):\n                    raise ValueError(\"NaN in transformed coordinates!\")\n\n                area, _ = geod.polygon_area_perimeter(lons, lats)\n                total_area += abs(area)\n\n            count_pixels = len(rows)\n            results[label] = {\n                \"area_m2\": total_area,\n                \"area_ha\": total_area / 10_000,\n                \"area_km2\": total_area / 1_000_000,\n                \"num_pixels\": count_pixels\n            }\n        return results\n\n    if isinstance(diff_masks, dict):\n        all_results = {}\n        for period_name, mask in diff_masks.items():\n            all_results[period_name] = process_single_mask(mask)\n        return all_results\n    else:\n        return process_single_mask(diff_masks)\n</pre> def compute_land_area_changes(diff_masks, transform, source_crs=\"EPSG:32616\"):     geod = Geod(ellps=\"WGS84\")     transformer = Transformer.from_crs(source_crs, \"EPSG:4326\", always_xy=True)          def process_single_mask(diff_mask):         results = {}         for value, label in [(-1, \"lost_water\"), (1, \"new_water\")]:             rows, cols = np.where(diff_mask == value)             total_area = 0.0              for row, col in zip(rows, cols):                 x_left, y_top = transform * (col, row)                 x_right, y_bottom = transform * (col + 1, row + 1)                  lon_left, lat_top = transformer.transform(x_left, y_top)                 lon_right, lat_bottom = transformer.transform(x_right, y_bottom)                  lons = [lon_left, lon_right, lon_right, lon_left]                 lats = [lat_top, lat_top, lat_bottom, lat_bottom]                  if any(np.isnan(lons)) or any(np.isnan(lats)):                     raise ValueError(\"NaN in transformed coordinates!\")                  area, _ = geod.polygon_area_perimeter(lons, lats)                 total_area += abs(area)              count_pixels = len(rows)             results[label] = {                 \"area_m2\": total_area,                 \"area_ha\": total_area / 10_000,                 \"area_km2\": total_area / 1_000_000,                 \"num_pixels\": count_pixels             }         return results      if isinstance(diff_masks, dict):         all_results = {}         for period_name, mask in diff_masks.items():             all_results[period_name] = process_single_mask(mask)         return all_results     else:         return process_single_mask(diff_masks) In\u00a0[\u00a0]: Copied! <pre>def create_land_change_plot(change_data):\n    periods = list(change_data.keys())\n    lost = [change_data[period]['lost_water']['area_km2'] for period in periods]\n    new = [change_data[period]['new_water']['area_km2'] for period in periods]\n\n    fig = go.Figure()\n\n    fig.add_trace(go.Bar(\n        x=periods,\n        y=lost,\n        name='Ods\u0142oni\u0119ty l\u0105d (km\u00b2)',\n        marker_color='red'\n    ))\n    fig.add_trace(go.Bar(\n        x=periods,\n        y=new,\n        name='Utracony l\u0105d (km\u00b2)',\n        marker_color='blue'\n    ))\n    fig.update_layout(\n        title='Zmiany powierzchni l\u0105dowej',\n        xaxis_title='Okres',\n        yaxis_title='Powierzchnia (km\u00b2)',\n        barmode='group'\n    )\n\n    return fig\n</pre> def create_land_change_plot(change_data):     periods = list(change_data.keys())     lost = [change_data[period]['lost_water']['area_km2'] for period in periods]     new = [change_data[period]['new_water']['area_km2'] for period in periods]      fig = go.Figure()      fig.add_trace(go.Bar(         x=periods,         y=lost,         name='Ods\u0142oni\u0119ty l\u0105d (km\u00b2)',         marker_color='red'     ))     fig.add_trace(go.Bar(         x=periods,         y=new,         name='Utracony l\u0105d (km\u00b2)',         marker_color='blue'     ))     fig.update_layout(         title='Zmiany powierzchni l\u0105dowej',         xaxis_title='Okres',         yaxis_title='Powierzchnia (km\u00b2)',         barmode='group'     )      return fig In\u00a0[\u00a0]: Copied! <pre>def generate_heatmap_data(mask: np.ndarray, transform: Affine, with_weights: bool = False):\n\n    rows, cols = np.where(mask != 0)\n\n    if rows.size == 0:\n        return []  # brak punkt\u00f3w\n\n    lons = []\n    lats = []\n    for row, col in zip(rows, cols):\n        lon, lat = transform * (col, row)\n        lons.append(lon)\n        lats.append(lat)\n\n    if with_weights:\n        weights = mask[rows, cols].astype(float)\n        heat_data = list(zip(lats, lons, weights))\n    else:\n        heat_data = list(zip(lats, lons))\n\n    return heat_data\n</pre> def generate_heatmap_data(mask: np.ndarray, transform: Affine, with_weights: bool = False):      rows, cols = np.where(mask != 0)      if rows.size == 0:         return []  # brak punkt\u00f3w      lons = []     lats = []     for row, col in zip(rows, cols):         lon, lat = transform * (col, row)         lons.append(lon)         lats.append(lat)      if with_weights:         weights = mask[rows, cols].astype(float)         heat_data = list(zip(lats, lons, weights))     else:         heat_data = list(zip(lats, lons))      return heat_data In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>############################# STREAMLIT INTERFACE ############################## --- KONFIGURACJA I DANE ---</p> In\u00a0[\u00a0]: Copied! <pre>st.sidebar.header(\"Ustawienia Obszaru Zainteresowania (AOI)\")\n</pre> st.sidebar.header(\"Ustawienia Obszaru Zainteresowania (AOI)\") In\u00a0[\u00a0]: Copied! <pre>locations = {\n    \"Luizjana (USA, delta Missisipi)\": {\"coords\": (29.17, -89.31)},\n    \"\u00c9tretat (Francja, klify)\": {\"coords\": (49.70, 0.19)},\n    \"Jezioro Aralskie (Kazachstan / Uzbekistan)\": {\"coords\": (46, 59.2)},\n    'Anglia (GB, klify)':{'coords': (53.87, -0.04)},\n}\n</pre> locations = {     \"Luizjana (USA, delta Missisipi)\": {\"coords\": (29.17, -89.31)},     \"\u00c9tretat (Francja, klify)\": {\"coords\": (49.70, 0.19)},     \"Jezioro Aralskie (Kazachstan / Uzbekistan)\": {\"coords\": (46, 59.2)},     'Anglia (GB, klify)':{'coords': (53.87, -0.04)}, } In\u00a0[\u00a0]: Copied! <pre>for name, info in locations.items():\n    lat, lon = info[\"coords\"]\n    if st.sidebar.button(f\"\ud83d\udccd {name}\"):\n        st.session_state[\"latitude\"] = lat\n        st.session_state[\"longitude\"] = lon\n        st.session_state['map_ready'] = False  #reset mapy zmian po zmianie lokalizacji\n</pre> for name, info in locations.items():     lat, lon = info[\"coords\"]     if st.sidebar.button(f\"\ud83d\udccd {name}\"):         st.session_state[\"latitude\"] = lat         st.session_state[\"longitude\"] = lon         st.session_state['map_ready'] = False  #reset mapy zmian po zmianie lokalizacji In\u00a0[\u00a0]: Copied! <pre>draw_aoi_enabled = st.sidebar.checkbox(\"Rysuj AOI na mapie\")\n</pre> draw_aoi_enabled = st.sidebar.checkbox(\"Rysuj AOI na mapie\") In\u00a0[\u00a0]: Copied! <pre>default_lat = st.session_state.get(\"latitude\", \"\")\ndefault_lon = st.session_state.get(\"longitude\", \"\")\n</pre> default_lat = st.session_state.get(\"latitude\", \"\") default_lon = st.session_state.get(\"longitude\", \"\") In\u00a0[\u00a0]: Copied! <pre>lat_input = st.sidebar.text_input(\"Szeroko\u015b\u0107 geograficzna (Lat)\", value=default_lat)\nlon_input = st.sidebar.text_input(\"D\u0142ugo\u015b\u0107 geograficzna (Lon)\", value=default_lon)\n</pre> lat_input = st.sidebar.text_input(\"Szeroko\u015b\u0107 geograficzna (Lat)\", value=default_lat) lon_input = st.sidebar.text_input(\"D\u0142ugo\u015b\u0107 geograficzna (Lon)\", value=default_lon) In\u00a0[\u00a0]: Copied! <pre>try:\n    latitude = float(lat_input)\n    longitude = float(lon_input)\n    cords_input = [latitude, longitude]\nexcept ValueError:\n    cords_input = None\n    if lat_input or lon_input:\n        st.sidebar.warning(\"Prosz\u0119 poda\u0107 poprawne liczby dla szeroko\u015bci i d\u0142ugo\u015bci geograficznej.\")\n</pre> try:     latitude = float(lat_input)     longitude = float(lon_input)     cords_input = [latitude, longitude] except ValueError:     cords_input = None     if lat_input or lon_input:         st.sidebar.warning(\"Prosz\u0119 poda\u0107 poprawne liczby dla szeroko\u015bci i d\u0142ugo\u015bci geograficznej.\") In\u00a0[\u00a0]: Copied! <pre>if st.sidebar.button(\"Ustaw wsp\u00f3\u0142rz\u0119dne AOI\"):\n    if cords_input is not None:\n        st.session_state[\"latitude\"] = latitude\n        st.session_state[\"longitude\"] = longitude\n        st.session_state['map_ready'] = False  #reset mapy zmian po zmianie lokalizacji\n    else:\n        st.sidebar.error(\"Niepoprawne wsp\u00f3\u0142rz\u0119dne. Prosz\u0119 poprawi\u0107 wpis.\")\n</pre> if st.sidebar.button(\"Ustaw wsp\u00f3\u0142rz\u0119dne AOI\"):     if cords_input is not None:         st.session_state[\"latitude\"] = latitude         st.session_state[\"longitude\"] = longitude         st.session_state['map_ready'] = False  #reset mapy zmian po zmianie lokalizacji     else:         st.sidebar.error(\"Niepoprawne wsp\u00f3\u0142rz\u0119dne. Prosz\u0119 poprawi\u0107 wpis.\") In\u00a0[\u00a0]: Copied! <pre># --- WYB\u00d3R LAT DO ANALIZY ---\nst.sidebar.header(\"Wyb\u00f3r Lat\")\nselected_years = st.sidebar.multiselect(\n    \"Wybierz lata do analizy zmian (min. 2 lata)\",\n    options=[2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024],\n    default=[2016, 2021, 2023]\n)\n</pre> # --- WYB\u00d3R LAT DO ANALIZY --- st.sidebar.header(\"Wyb\u00f3r Lat\") selected_years = st.sidebar.multiselect(     \"Wybierz lata do analizy zmian (min. 2 lata)\",     options=[2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024],     default=[2016, 2021, 2023] ) In\u00a0[\u00a0]: Copied! <pre>if 2022 in selected_years:\n    st.sidebar.warning(\"Rok 2022 mo\u017ce zawiera\u0107 anomalie w danych.\")\n</pre> if 2022 in selected_years:     st.sidebar.warning(\"Rok 2022 mo\u017ce zawiera\u0107 anomalie w danych.\") In\u00a0[\u00a0]: Copied! <pre># --- GENEROWANIE MAPY ZMIAN ---\nif st.sidebar.button(\"Generuj map\u0119 zmian\"):\n    if len(selected_years) &lt; 2:\n        st.error(\"Prosz\u0119 wybra\u0107 co najmniej dwa lata, aby obliczy\u0107 zmiany.\")\n    elif cords_input is None:\n        st.error(\"Prosz\u0119 poda\u0107 poprawne wsp\u00f3\u0142rz\u0119dne AOI.\")\n    else:\n        with st.spinner(\"Przetwarzanie danych satelitarnych i obliczanie zmian... To mo\u017ce potrwa\u0107 kilka minut.\"):\n            results = process_data_and_differences(cords_input, selected_years)\n\n            if results:\n                water_mask_clipped_by_year, water_mask_differences, bounds, aoi_minx, aoi_miny, aoi_maxx, aoi_maxy, red_transform_global = results\n\n                image_colored_paths_by_diff = generate_difference_image_overlays(water_mask_differences)\n                \n                st.session_state['map_ready'] = True\n                st.session_state['water_mask_differences'] = water_mask_differences\n                st.session_state['image_paths'] = image_colored_paths_by_diff\n                st.session_state['aoi_bounds'] = (aoi_minx, aoi_miny, aoi_maxx, aoi_maxy)\n                st.session_state['red_transform'] = red_transform_global\n            else:\n                st.error(\"Nie uda\u0142o si\u0119 wygenerowa\u0107 mapy zmian.\")\n</pre> # --- GENEROWANIE MAPY ZMIAN --- if st.sidebar.button(\"Generuj map\u0119 zmian\"):     if len(selected_years) &lt; 2:         st.error(\"Prosz\u0119 wybra\u0107 co najmniej dwa lata, aby obliczy\u0107 zmiany.\")     elif cords_input is None:         st.error(\"Prosz\u0119 poda\u0107 poprawne wsp\u00f3\u0142rz\u0119dne AOI.\")     else:         with st.spinner(\"Przetwarzanie danych satelitarnych i obliczanie zmian... To mo\u017ce potrwa\u0107 kilka minut.\"):             results = process_data_and_differences(cords_input, selected_years)              if results:                 water_mask_clipped_by_year, water_mask_differences, bounds, aoi_minx, aoi_miny, aoi_maxx, aoi_maxy, red_transform_global = results                  image_colored_paths_by_diff = generate_difference_image_overlays(water_mask_differences)                                  st.session_state['map_ready'] = True                 st.session_state['water_mask_differences'] = water_mask_differences                 st.session_state['image_paths'] = image_colored_paths_by_diff                 st.session_state['aoi_bounds'] = (aoi_minx, aoi_miny, aoi_maxx, aoi_maxy)                 st.session_state['red_transform'] = red_transform_global             else:                 st.error(\"Nie uda\u0142o si\u0119 wygenerowa\u0107 mapy zmian.\") In\u00a0[\u00a0]: Copied! <pre>#je\u015bli mapa zosta\u0142a wygenerowana wczesniej to jest przechowywana w sesji i jest automatucznie wy\u015bwietlana\nif st.session_state.get('map_ready'):\n    aoi_minx, aoi_miny, aoi_maxx, aoi_maxy = st.session_state['aoi_bounds']\n    image_colored_paths_by_diff = st.session_state['image_paths']\n    red_transform_global = st.session_state['red_transform']\n    water_mask_differences = st.session_state['water_mask_differences']\n\n    land_area_changes = compute_land_area_changes(water_mask_differences, red_transform_global)\n    print(f\"Zmiany powierzchni l\u0105dowej: {land_area_changes}\") #to do debugowania\n\n    fig = create_land_change_plot(land_area_changes)\n\n    m = folium.Map(location=[(aoi_miny + aoi_maxy) / 2, (aoi_minx + aoi_maxx) / 2], zoom_start=12)\n\n    for diff_label, img_path in image_colored_paths_by_diff.items():\n        overlay = ImageOverlay(\n            image=img_path,\n            bounds=[[aoi_miny, aoi_minx], [aoi_maxy, aoi_maxx]],\n            opacity=0.8,\n            name=f\"Zmiany: {diff_label}\"\n        )\n        overlay.add_to(m)\n\n    folium.Polygon(\n        locations=[(aoi_miny, aoi_minx), (aoi_miny, aoi_maxx), (aoi_maxy, aoi_maxx), (aoi_maxy, aoi_minx), (aoi_miny, aoi_minx)],\n        color='blue',\n        weight=2,\n        fill=False,\n        popup='Obszar Zainteresowania (AOI)'\n    ).add_to(m)\n\n    folium.LayerControl().add_to(m)\n\n    legend_html = \"\"\"\n        &lt;div style=\"\n            position: fixed; \n            bottom: 30px; left: 30px; width: 180px; height: 85px; \n            background-color: white;\n            border:2px solid grey; \n            z-index:9999;\n            font-size:14px;\n            padding: 10px;\n        \"&gt;\n        &lt;b&gt;Legenda zmian&lt;/b&gt;&lt;br&gt;\n        &lt;i style=\"background: rgba(255,0,0,0.78); width: 15px; height: 15px; display: inline-block;\"&gt;&lt;/i&gt;\n        Ods\u0142oni\u0119ty l\u0105d&lt;br&gt;\n        &lt;i style=\"background: rgba(0,0,255,0.78); width: 15px; height: 15px; display: inline-block;\"&gt;&lt;/i&gt;\n        Utracony l\u0105d\n        &lt;/div&gt;\n    \"\"\"\n\n    m.get_root().html.add_child(folium.Element(legend_html))\n\n    full_map_html = m.get_root().render()\n    st.components.v1.html(full_map_html, height=700, scrolling=False)\n\n    st.plotly_chart(fig, use_container_width=True)\n\n    if st.button(\"Wyczy\u015b\u0107 map\u0119 zmian\"):\n        st.session_state['map_ready'] = False\n        st.session_state.pop('image_paths', None)\n        st.session_state.pop('aoi_bounds', None)\n\n\n\n\n    #opcja eksportu obrazu w formie pliku zip\n    if image_colored_paths_by_diff:\n        with tempfile.NamedTemporaryFile(suffix=\".zip\", delete=False) as tmp_zip:\n            with zipfile.ZipFile(tmp_zip.name, 'w') as zipf:\n                for label, img_path in image_colored_paths_by_diff.items():\n                    arcname = f\"{label}.png\"\n                    zipf.write(img_path, arcname=arcname)\n\n        with open(tmp_zip.name, \"rb\") as f:\n            with st.sidebar:\n                st.markdown(\"---\")\n                st.subheader(\"Eksport zmian wodnych\")\n                st.download_button(\n                    label=\"Pobierz wszystkie obrazy zmian (ZIP)\",\n                    data=f,\n                    file_name=\"zmiany_wodne.zip\",\n                    mime=\"application/zip\"\n                )\nif not st.session_state.get('map_ready', False):\n    default_location = [45, 0]\n    default_zoom = 2\n\n    if \"latitude\" in st.session_state and \"longitude\" in st.session_state:\n        center_lat = st.session_state[\"latitude\"]\n        center_lon = st.session_state[\"longitude\"]\n        zoom_level = 10\n    else:\n        center_lat, center_lon = default_location\n        zoom_level = default_zoom\n\n    m_base = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_level)\n\n    for name, info in locations.items():\n        lat, lon = info[\"coords\"]\n        folium.Marker(\n            location=[lat, lon],\n            tooltip=name,\n            icon=folium.Icon(color=\"green\", icon=\"info-sign\")\n        ).add_to(m_base)\n\n    if cords_input:\n        folium.CircleMarker(\n            location=cords_input,\n            radius=6,\n            color=\"blue\",\n            fill=True,\n            fill_color=\"blue\",\n            popup=\"Wybrane wsp\u00f3\u0142rz\u0119dne\"\n        ).add_to(m_base)\n\n    if draw_aoi_enabled:\n        draw = Draw(\n            draw_options={\n                \"rectangle\": True,\n                \"polygon\": False,\n                \"circle\": False,\n                \"marker\": False,\n                \"circlemarker\": False,\n            },\n            edit_options={\"edit\": False},\n        )\n        draw.add_to(m_base)\n\n    result = st_folium(m_base, height=700, width=1400)\n\n    #obs\u0142uga rysowania AOI przez u\u017cytkownika\n    if draw_aoi_enabled and result and \"last_active_drawing\" in result and result[\"last_active_drawing\"]:\n        geo = result[\"last_active_drawing\"][\"geometry\"]\n        coords = geo[\"coordinates\"][0]\n        lats = [c[1] for c in coords]\n        lons = [c[0] for c in coords]\n\n        lat_center = np.mean(lats)\n        lon_center = np.mean(lons)\n\n        st.session_state[\"latitude\"] = lat_center\n        st.session_state[\"longitude\"] = lon_center\n\n        st.success(f\"Wybrano obszar AOI: ({lat_center:.4f}, {lon_center:.4f})\")\n</pre> #je\u015bli mapa zosta\u0142a wygenerowana wczesniej to jest przechowywana w sesji i jest automatucznie wy\u015bwietlana if st.session_state.get('map_ready'):     aoi_minx, aoi_miny, aoi_maxx, aoi_maxy = st.session_state['aoi_bounds']     image_colored_paths_by_diff = st.session_state['image_paths']     red_transform_global = st.session_state['red_transform']     water_mask_differences = st.session_state['water_mask_differences']      land_area_changes = compute_land_area_changes(water_mask_differences, red_transform_global)     print(f\"Zmiany powierzchni l\u0105dowej: {land_area_changes}\") #to do debugowania      fig = create_land_change_plot(land_area_changes)      m = folium.Map(location=[(aoi_miny + aoi_maxy) / 2, (aoi_minx + aoi_maxx) / 2], zoom_start=12)      for diff_label, img_path in image_colored_paths_by_diff.items():         overlay = ImageOverlay(             image=img_path,             bounds=[[aoi_miny, aoi_minx], [aoi_maxy, aoi_maxx]],             opacity=0.8,             name=f\"Zmiany: {diff_label}\"         )         overlay.add_to(m)      folium.Polygon(         locations=[(aoi_miny, aoi_minx), (aoi_miny, aoi_maxx), (aoi_maxy, aoi_maxx), (aoi_maxy, aoi_minx), (aoi_miny, aoi_minx)],         color='blue',         weight=2,         fill=False,         popup='Obszar Zainteresowania (AOI)'     ).add_to(m)      folium.LayerControl().add_to(m)      legend_html = \"\"\"          Legenda zmian          Ods\u0142oni\u0119ty l\u0105d          Utracony l\u0105d              \"\"\"      m.get_root().html.add_child(folium.Element(legend_html))      full_map_html = m.get_root().render()     st.components.v1.html(full_map_html, height=700, scrolling=False)      st.plotly_chart(fig, use_container_width=True)      if st.button(\"Wyczy\u015b\u0107 map\u0119 zmian\"):         st.session_state['map_ready'] = False         st.session_state.pop('image_paths', None)         st.session_state.pop('aoi_bounds', None)         #opcja eksportu obrazu w formie pliku zip     if image_colored_paths_by_diff:         with tempfile.NamedTemporaryFile(suffix=\".zip\", delete=False) as tmp_zip:             with zipfile.ZipFile(tmp_zip.name, 'w') as zipf:                 for label, img_path in image_colored_paths_by_diff.items():                     arcname = f\"{label}.png\"                     zipf.write(img_path, arcname=arcname)          with open(tmp_zip.name, \"rb\") as f:             with st.sidebar:                 st.markdown(\"---\")                 st.subheader(\"Eksport zmian wodnych\")                 st.download_button(                     label=\"Pobierz wszystkie obrazy zmian (ZIP)\",                     data=f,                     file_name=\"zmiany_wodne.zip\",                     mime=\"application/zip\"                 ) if not st.session_state.get('map_ready', False):     default_location = [45, 0]     default_zoom = 2      if \"latitude\" in st.session_state and \"longitude\" in st.session_state:         center_lat = st.session_state[\"latitude\"]         center_lon = st.session_state[\"longitude\"]         zoom_level = 10     else:         center_lat, center_lon = default_location         zoom_level = default_zoom      m_base = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_level)      for name, info in locations.items():         lat, lon = info[\"coords\"]         folium.Marker(             location=[lat, lon],             tooltip=name,             icon=folium.Icon(color=\"green\", icon=\"info-sign\")         ).add_to(m_base)      if cords_input:         folium.CircleMarker(             location=cords_input,             radius=6,             color=\"blue\",             fill=True,             fill_color=\"blue\",             popup=\"Wybrane wsp\u00f3\u0142rz\u0119dne\"         ).add_to(m_base)      if draw_aoi_enabled:         draw = Draw(             draw_options={                 \"rectangle\": True,                 \"polygon\": False,                 \"circle\": False,                 \"marker\": False,                 \"circlemarker\": False,             },             edit_options={\"edit\": False},         )         draw.add_to(m_base)      result = st_folium(m_base, height=700, width=1400)      #obs\u0142uga rysowania AOI przez u\u017cytkownika     if draw_aoi_enabled and result and \"last_active_drawing\" in result and result[\"last_active_drawing\"]:         geo = result[\"last_active_drawing\"][\"geometry\"]         coords = geo[\"coordinates\"][0]         lats = [c[1] for c in coords]         lons = [c[0] for c in coords]          lat_center = np.mean(lats)         lon_center = np.mean(lons)          st.session_state[\"latitude\"] = lat_center         st.session_state[\"longitude\"] = lon_center          st.success(f\"Wybrano obszar AOI: ({lat_center:.4f}, {lon_center:.4f})\")"},{"location":"stac/","title":"\ud83d\udc7d STAC","text":"In\u00a0[\u00a0]: Copied! <pre>import pystac_client\nimport planetary_computer as pc\nimport rasterio\nimport numpy as np\nimport folium\nfrom folium.raster_layers import ImageOverlay\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom rasterio.mask import mask\nfrom rasterio.warp import transform_geom\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport io\nimport base64\n</pre> import pystac_client import planetary_computer as pc import rasterio import numpy as np import folium from folium.raster_layers import ImageOverlay import matplotlib.pyplot as plt from PIL import Image from rasterio.mask import mask from rasterio.warp import transform_geom import matplotlib.pyplot as plt import matplotlib.colors as mcolors import io import base64 In\u00a0[\u00a0]: Copied! <pre># Po\u0142\u0105czenie z publicznym katalogiem STAC na Azure Planetary Computer\nstac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\nstac_client = pystac_client.Client.open(stac_url)\n\n# Lista dost\u0119pnych kolekcji\ncollections = stac_client.get_all_collections()\nprint(\"Dost\u0119pne kolekcje:\")\nfor collection in collections:\n    print(collection.id, \"-\", collection.title)\n</pre> # Po\u0142\u0105czenie z publicznym katalogiem STAC na Azure Planetary Computer stac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\" stac_client = pystac_client.Client.open(stac_url)  # Lista dost\u0119pnych kolekcji collections = stac_client.get_all_collections() print(\"Dost\u0119pne kolekcje:\") for collection in collections:     print(collection.id, \"-\", collection.title) In\u00a0[\u00a0]: Copied! <pre># Definiowanie obszaru zainteresowania i przedzia\u0142u czasowego\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[\n        [19.86, 50.02], [20.03, 50.02], [20.03, 50.12], [19.86, 50.12], [19.86, 50.02]\n    ]]\n}\ntime_range = \"2023-06-01/2023-06-30\"\n\n# Wyszukiwanie danych Sentinel-2 spe\u0142niaj\u0105cych kryteria\nsearch = stac_client.search(\n    collections=[\"sentinel-2-l2a\"],\n    intersects=aoi,\n    datetime=time_range,\n    max_items=5\n)\n\nitems = list(search.items())\nprint(f\"Znaleziono {len(items)} scen Sentinel-2\")\nprint(f\"Nazwy scen: {items}\")\n</pre> # Definiowanie obszaru zainteresowania i przedzia\u0142u czasowego aoi = {     \"type\": \"Polygon\",     \"coordinates\": [[         [19.86, 50.02], [20.03, 50.02], [20.03, 50.12], [19.86, 50.12], [19.86, 50.02]     ]] } time_range = \"2023-06-01/2023-06-30\"  # Wyszukiwanie danych Sentinel-2 spe\u0142niaj\u0105cych kryteria search = stac_client.search(     collections=[\"sentinel-2-l2a\"],     intersects=aoi,     datetime=time_range,     max_items=5 )  items = list(search.items()) print(f\"Znaleziono {len(items)} scen Sentinel-2\") print(f\"Nazwy scen: {items}\")  In\u00a0[\u00a0]: Copied! <pre>item = items[0]\n# Pobranie pasm B04 (czerwone) i B08 (NIR)\nb04_asset = pc.sign(item.assets[\"B04\"].href)\nb08_asset = pc.sign(item.assets[\"B08\"].href)\n</pre> item = items[0] # Pobranie pasm B04 (czerwone) i B08 (NIR) b04_asset = pc.sign(item.assets[\"B04\"].href) b08_asset = pc.sign(item.assets[\"B08\"].href) In\u00a0[\u00a0]: Copied! <pre># Przyci\u0119cie zobrazowania do AOI\ndef clip_raster(dataset, aoi):\n    from shapely.geometry import shape\n    import json\n    \n    # Konwersja AOI do uk\u0142adu wsp\u00f3\u0142rz\u0119dnych rastra\n    aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))\n    \n    aoi_geom = [json.loads(json.dumps(aoi_transformed))]\n    clipped_array, clipped_transform = mask(dataset, [shape(aoi_transformed)], crop=True)\n    return clipped_array[0], clipped_transform\n    from shapely.geometry import shape\n    import json\n    aoi_geom = [json.loads(json.dumps(aoi))]\n    clipped_array, clipped_transform = mask(dataset, [shape(aoi)], crop=True)\n    return clipped_array[0], clipped_transform\n\n# Otwieranie i przycinanie pasm\nwith rasterio.open(b04_asset) as red_ds, rasterio.open(b08_asset) as nir_ds:\n    red, red_transform = clip_raster(red_ds, aoi)\n    nir, nir_transform = clip_raster(nir_ds, aoi)\n    red = red.astype(np.float32)\n    nir = nir.astype(np.float32)\n    ndvi = (nir - red) / (nir + red + 1e-10)\n    from rasterio.warp import transform_bounds\n\n# Pobranie uk\u0142adu wsp\u00f3\u0142rz\u0119dnych pliku\ndataset_crs = red_ds.crs\n\n# Przekszta\u0142cenie granic obrazu do WGS84\nheight, width = red.shape\n# Obliczenie granic po przyci\u0119ciu\nleft, top = red_transform * (0, 0)  # Lewy g\u00f3rny r\u00f3g\nright, bottom = red_transform * (width, height)  # Prawy dolny r\u00f3g\nbounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top)\n</pre> # Przyci\u0119cie zobrazowania do AOI def clip_raster(dataset, aoi):     from shapely.geometry import shape     import json          # Konwersja AOI do uk\u0142adu wsp\u00f3\u0142rz\u0119dnych rastra     aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))          aoi_geom = [json.loads(json.dumps(aoi_transformed))]     clipped_array, clipped_transform = mask(dataset, [shape(aoi_transformed)], crop=True)     return clipped_array[0], clipped_transform     from shapely.geometry import shape     import json     aoi_geom = [json.loads(json.dumps(aoi))]     clipped_array, clipped_transform = mask(dataset, [shape(aoi)], crop=True)     return clipped_array[0], clipped_transform  # Otwieranie i przycinanie pasm with rasterio.open(b04_asset) as red_ds, rasterio.open(b08_asset) as nir_ds:     red, red_transform = clip_raster(red_ds, aoi)     nir, nir_transform = clip_raster(nir_ds, aoi)     red = red.astype(np.float32)     nir = nir.astype(np.float32)     ndvi = (nir - red) / (nir + red + 1e-10)     from rasterio.warp import transform_bounds  # Pobranie uk\u0142adu wsp\u00f3\u0142rz\u0119dnych pliku dataset_crs = red_ds.crs  # Przekszta\u0142cenie granic obrazu do WGS84 height, width = red.shape # Obliczenie granic po przyci\u0119ciu left, top = red_transform * (0, 0)  # Lewy g\u00f3rny r\u00f3g right, bottom = red_transform * (width, height)  # Prawy dolny r\u00f3g bounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top) In\u00a0[\u00a0]: Copied! <pre># Normalizacja do zakresu 0-255\n# Tworzenie mapy kolor\u00f3w do wizualizacji NDVI\ncmap = plt.get_cmap('RdYlGn')  # Czerwony-\u017c\u00f3\u0142ty-zielony\nnorm = mcolors.Normalize(vmin=-1, vmax=1)\nndvi_colored = cmap(norm(ndvi))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa\nndvi_colored = (ndvi_colored * 255).astype(np.uint8)\n\n# Tworzenie obrazu NDVI\nimage = Image.fromarray(ndvi_colored, mode=\"RGB\")\nimage = image.convert(\"RGBA\")\n\n# Konwersja obrazu na format base64\nimage_buffer = io.BytesIO()\nimage.save(image_buffer, format='PNG')\nimage_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8')\nimport tempfile\n\n# Zapisanie obrazu NDVI do pliku tymczasowego\ntemp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\nimage.save(temp_file.name, format='PNG')\nimage_url = temp_file.name\n</pre> # Normalizacja do zakresu 0-255 # Tworzenie mapy kolor\u00f3w do wizualizacji NDVI cmap = plt.get_cmap('RdYlGn')  # Czerwony-\u017c\u00f3\u0142ty-zielony norm = mcolors.Normalize(vmin=-1, vmax=1) ndvi_colored = cmap(norm(ndvi))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa ndvi_colored = (ndvi_colored * 255).astype(np.uint8)  # Tworzenie obrazu NDVI image = Image.fromarray(ndvi_colored, mode=\"RGB\") image = image.convert(\"RGBA\")  # Konwersja obrazu na format base64 image_buffer = io.BytesIO() image.save(image_buffer, format='PNG') image_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8') import tempfile  # Zapisanie obrazu NDVI do pliku tymczasowego temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False) image.save(temp_file.name, format='PNG') image_url = temp_file.name In\u00a0[\u00a0]: Copied! <pre># Tworzenie mapy i dodanie NDVI jako warstwy rastrowej\nminx, miny, maxx, maxy = bounds\nm = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=10)\nimage_overlay = ImageOverlay(\n    image=image_url,\n    bounds=[[miny, minx], [maxy, maxx]],\n    opacity=0.6,\n    name=\"NDVI Layer\"\n)\nimage_overlay.add_to(m)\n\n# Dodanie opcji sterowania warstwami\nfolium.LayerControl().add_to(m)\n\n# Wy\u015bwietlenie mapy\nm\n</pre> # Tworzenie mapy i dodanie NDVI jako warstwy rastrowej minx, miny, maxx, maxy = bounds m = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=10) image_overlay = ImageOverlay(     image=image_url,     bounds=[[miny, minx], [maxy, maxx]],     opacity=0.6,     name=\"NDVI Layer\" ) image_overlay.add_to(m)  # Dodanie opcji sterowania warstwami folium.LayerControl().add_to(m)  # Wy\u015bwietlenie mapy m"},{"location":"stac/#stac","title":"STAC\u00b6","text":"<p>SpatioTemporal Asset Catalog (STAC) to otwarty standard s\u0142u\u017c\u0105cy do organizowania i przeszukiwania danych geoprzestrzennych, takich jak zobrazowania satelitarne, modele terenu czy dane LiDAR. STAC zosta\u0142 zaprojektowany z my\u015bl\u0105 o efektywnym katalogowaniu zasob\u00f3w przestrzennych, co pozwala na ich \u0142atwe odkrywanie, filtrowanie i analiz\u0119. Dzi\u0119ki STAC u\u017cytkownicy mog\u0105 szybko przeszukiwa\u0107 ogromne zbiory danych, stosuj\u0105c kryteria takie jak lokalizacja, przedzia\u0142 czasowy, rozdzielczo\u015b\u0107 czy dost\u0119pno\u015b\u0107 pasm spektralnych.</p> <p>Najwa\u017cniejsze zalety STAC:</p> <ul> <li>Standaryzacja \u2013 jednolity format umo\u017cliwia interoperacyjno\u015b\u0107 mi\u0119dzy r\u00f3\u017cnymi dostawcami danych.</li> <li>Elastyczno\u015b\u0107 \u2013 mo\u017cliwo\u015b\u0107 rozszerzania schemat\u00f3w metadanych i dostosowywania ich do specyficznych potrzeb.</li> <li>\u0141atwo\u015b\u0107 przeszukiwania \u2013 szybkie zapytania pozwalaj\u0105 na odnalezienie interesuj\u0105cych zobrazowa\u0144 na podstawie obszaru i daty.</li> <li>Integracja z chmur\u0105 \u2013 wiele publicznych katalog\u00f3w STAC dost\u0119pnych jest w chmurze, co u\u0142atwia prac\u0119 z du\u017cymi zbiorami danych.</li> </ul> <p>Dzi\u0119ki STAC eksploracja i analiza danych geoprzestrzennych staje si\u0119 znacznie bardziej efektywna, co ma kluczowe znaczenie dla naukowc\u00f3w, analityk\u00f3w oraz firm zajmuj\u0105cych si\u0119 obserwacj\u0105 Ziemi.</p>"},{"location":"stac/#przeszukanie-kolekcji-dostepnych-w-stac-w-planetary-computer","title":"Przeszukanie kolekcji dost\u0119pnych w STAC w Planetary Computer\u00b6","text":""},{"location":"stac/#przeszukiwanie-w-czasie-i-przestrzeni","title":"Przeszukiwanie w czasie i przestrzeni\u00b6","text":""},{"location":"stac/#przeprowadzanie-obliczen","title":"Przeprowadzanie oblicze\u0144\u00b6","text":""},{"location":"stac/#wyswietlanie-wynikow","title":"Wy\u015bwietlanie wynik\u00f3w\u00b6","text":""},{"location":"my_project/desc/","title":"\ud83d\udccb Opis","text":""},{"location":"my_project/desc/#cel","title":"Cel","text":""},{"location":"my_project/desc/#metody","title":"Metody","text":""},{"location":"my_project/desc/#techniczna-implementacja","title":"Techniczna implementacja","text":""},{"location":"my_project/desc/#_1","title":"...","text":""},{"location":"my_project/log/","title":"Log","text":""},{"location":"my_project/log/#28032025","title":"28.03.2025","text":"<p>Post\u0119p:</p> <p>Problemy / kwestie do przedyskutowania:</p>"},{"location":"my_project/log/#21032025","title":"21.03.2025","text":"<p>Post\u0119p:</p> <p>Problemy / kwestie do przedyskutowania:</p>"},{"location":"projects/ambulances/","title":"Analiza przestrzenna dla przejazd\u00f3w karetek w Ma\u0142opolsce","text":"<p>Opis problemu badawczego: System ratownictwa medycznego odgrywa kluczow\u0105 rol\u0119 w zapewnianiu szybkiej pomocy w nag\u0142ych wypadkach. Analiza tras przejazd\u00f3w karetek pozwala na identyfikacj\u0119 wzorc\u00f3w czasowych i przestrzennych, ocen\u0119 efektywno\u015bci dojazd\u00f3w oraz wskazanie obszar\u00f3w wymagaj\u0105cych usprawnienia. Celem projektu jest eksploracyjna analiza przejazd\u00f3w karetek w wojew\u00f3dztwie ma\u0142opolskim na podstawie danych GPS oraz badanie czynnik\u00f3w wp\u0142ywaj\u0105cych na czas reakcji s\u0142u\u017cb ratunkowych.</p> <p>\u0179r\u00f3d\u0142o danych - od prowadz\u0105cego</p> <p>Proponowane techniki analizy danych:</p> <ul> <li>Wst\u0119pna analiza danych GPS<ul> <li>Oczyszczenie i agregacja danych GPS dla poszczeg\u00f3lnych kurs\u00f3w</li> <li>Sprawdzenie kompletno\u015bci danych (np. braki w sygnale, nieprawid\u0142owe punkty)</li> </ul> </li> <li>Analiza czasowa i przestrzenna przejazd\u00f3w<ul> <li>\u015arednie czasy dojazdu do poszczeg\u00f3lnych miejscowo\u015bci / na poszczeg\u00f3lnych obszarach / w siatce H3</li> <li>Analiza godzin szczytu \u2013 czy w okre\u015blonych porach dnia wyst\u0119puj\u0105 wi\u0119ksze op\u00f3\u017anienia?</li> <li>Wizualizacja g\u0119sto\u015bci przejazd\u00f3w \u2013 heatmapy najcz\u0119\u015bciej ucz\u0119szczanych tras.</li> </ul> </li> <li>Wykrywanie anomalii i problematycznych obszar\u00f3w<ul> <li>Identyfikacja tras o nienaturalnie d\u0142ugich czasach przejazdu (np. korki, z\u0142a infrastruktura).</li> <li>Wykrycie cz\u0119sto omijanych obszar\u00f3w \u2013 mo\u017cliwe luki w pokryciu ratowniczym.</li> <li>Analiza wp\u0142ywu teren\u00f3w zurbanizowanych vs wiejskich na efektywno\u015b\u0107 dojazdu.</li> </ul> </li> <li>?Modelowanie predykcyjne czasu dojazdu?<ul> <li>Regresja liniowa/random forest \u2013 przewidywanie czasu przejazdu na podstawie odleg\u0142o\u015bci, godziny, warunk\u00f3w drogowych.</li> <li>Modelowanie szereg\u00f3w czasowych (ARIMA, Prophet) \u2013 przewidywanie nat\u0119\u017cenia przejazd\u00f3w w przysz\u0142ych okresach.</li> <li>Analiza sieciowa (graph analysis) \u2013 optymalizacja tras na podstawie danych OSM i sieci dr\u00f3g.</li> </ul> </li> </ul>"},{"location":"projects/anomalies_sentinel2/","title":"Wykrywanie anomalii na obrazach Sentinel-2","text":"<p>Opis problemu badawczego: Dane optyczne pozwalaj\u0105 na monitorowanie zmian w \u015brodowisku, ale detekcja nietypowych zdarze\u0144 (anomalii) wymaga zaawansowanej analizy danych. Anomalie mog\u0105 obejmowa\u0107 wylesienia, susze, po\u017cary, zmiany pokrycia terenu, zanieczyszczenia w\u00f3d i anomalia w ro\u015blinno\u015bci. Celem projektu jest opracowanie systemu automatycznego wykrywania anomalii na podstawie wska\u017anik\u00f3w spektralnych i metod uczenia maszynowego, w tym algorytm\u00f3w nadzorowanych i nienadzorowanych.</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>Microsoft Planetary Computer</li> <li>Earth Search by Element 84 (AWS Registry of Open Data)</li> <li>Google Earth Engine</li> <li>Copernicus Data Space Ecosystem</li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Wst\u0119pne przetwarzanie - korekcja atmosferyczna, maskowanie chmur, przyci\u0119cie do AOI</li> <li>Ekstrakcja cech spektralnych - obliczenie wska\u017anik\u00f3w spektralnych (np. NDVI czy MSAVI - zmiany wegetacji, NDWI - zmiany dot. zasi\u0119gu w\u00f3d powierzchniowych, NBR - wykrywanie po\u017car\u00f3w)</li> <li>Wykrywanie anomalii<ul> <li>Metody statystyczne (regu\u0142y odstaj\u0105ce):<ul> <li>Z-score \u2013 oznaczanie warto\u015bci odleg\u0142ych od \u015bredniej o wi\u0119cej ni\u017c X odchyle\u0144 standardowych</li> <li>Percentylowa analiza progowa \u2013 wykrywanie warto\u015bci powy\u017cej 95. lub poni\u017cej 5. percentyla dla danego obszaru i wska\u017anika spektralnego</li> <li>Metody bazuj\u0105ce na g\u0142\u00f3wnych sk\u0142adowych (PCA) \u2013 analiza kierunk\u00f3w najwi\u0119kszej wariancji i identyfikacja punkt\u00f3w odstaj\u0105cych</li> </ul> </li> <li>Metody wykrywania anomalii oparte na ML (unsupervised learning):<ul> <li>Isolation Forest \u2013 model izoluj\u0105cy nietypowe obserwacje (np. nag\u0142e zmiany NDVI po wylesieniu).</li> <li>Local Outlier Factor (LOF) \u2013 wykrywanie anomalii poprzez analiz\u0119 g\u0119sto\u015bci punkt\u00f3w w przestrzeni cech spektralnych.</li> <li>DBSCAN \u2013 grupowanie anomalii na podstawie g\u0119sto\u015bci obserwacji (pozwala wykrywa\u0107 lokalne zmiany w \u015brodowisku).</li> <li>Autoenkodery (AE) \u2013 redukcja wymiarowo\u015bci i wykrywanie nietypowych pikseli poprzez r\u00f3\u017cnic\u0119 mi\u0119dzy rekonstrukcj\u0105 a rzeczywistym obrazem.</li> </ul> </li> </ul> </li> </ul>"},{"location":"projects/insar/","title":"Wykrywanie zmian wysoko\u015bci terenu na podstawie danych SAR","text":"<p>Opis problemu badawczego: Zmiany wysoko\u015bci terenu mog\u0105 by\u0107 wynikiem zjawisk geologicznych (trz\u0119sienia ziemi, wulkany, osuwiska), dzia\u0142a\u0144 cz\u0142owieka (eksploatacja g\u00f3rnicza, zapadliska, urbanizacja) lub proces\u00f3w naturalnych (erozja, topnienie lodowc\u00f3w). Celem projektu jest monitorowanie przemieszcze\u0144 powierzchni ziemi w czasie, z wykorzystaniem danych SAR z r\u00f3\u017cnych misji radarowych. Analiza b\u0119dzie oparta na technikach interferometrycznych (InSAR).</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>Sentinel-1 poprzez Copernicus Data Space Ecosystem / Microsoft Planetary Computer / Earth Search by Element 84 (AWS Registry of Open Data)</li> <li>Programy otwartych danych od komercyjnych dostawc\u00f3w (ICEYE, Umbra, Capella Space)</li> </ul> <p>Dodatkowe informacje:</p> <ul> <li>Biblioteki dedykowane dla analiz na danych SAR to m.in. <code>PyGMTSAR</code> czy <code>MintPy</code></li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Wyb\u00f3r analizowanego epizodu (np. wulkan Fernandina - Gal\u00e1pagos Islands, Ecuador)</li> <li>Korekcja geometryczna i radiometryczna</li> <li>Tworzenie interferogram\u00f3w \u2013 por\u00f3wnanie fazy fali radarowej</li> <li>Persistent Scatterer Interferometry (PS-InSAR) \u2013 analiza d\u0142ugoterminowych zmian wysoko\u015bci</li> <li>Small Baseline Subset (SBAS) \u2013 kr\u00f3tkoterminowe zmiany</li> <li>Konwersja r\u00f3\u017cnic fazowych na warto\u015bci wysoko\u015bciowe</li> <li>Wizualizacja - mapy deformacji, wykresy zmian</li> </ul>"},{"location":"projects/lulc/","title":"Analiza zmian w u\u017cytkowaniu terenu dla wybranego obszaru na podstawie gotowych klasyfikacji","text":"<p>Opis problemu badawczego: Zmiany w u\u017cytkowaniu terenu odgrywaj\u0105 kluczow\u0105 rol\u0119 w planowaniu przestrzennym, zarz\u0105dzaniu zasobami naturalnymi i analizie wp\u0142ywu cz\u0142owieka na \u015brodowisko. Dzi\u0119ki gotowym produktom klasyfikacji u\u017cytkowania terenu, mo\u017cliwa jest analiza d\u0142ugoterminowych trend\u00f3w zmian krajobrazu. Celem projektu jest identyfikacja zmian w u\u017cytkowaniu terenu w wybranym regionie na podstawie gotowych danych klasyfikacyjnych.</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>CORINE Land Cover (CLC)</li> <li>Copernicus Global Land Cover</li> <li>ESA WorldCover</li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Statystyczna analiza trend\u00f3w u\u017cytkowania terenu \u2013 obliczenie powierzchni poszczeg\u00f3lnych klas dla r\u00f3\u017cnych lat</li> <li>Wizualizacja zmian w postaci map i wykres\u00f3w \u2013 interaktywne mapy r\u00f3\u017cnic u\u017cytkowania terenu</li> <li>Okre\u015blenie hotspot\u00f3w - obszar\u00f3w, gdzie wyst\u0119puje du\u017ca ilo\u015b\u0107 zmian</li> <li>?Modelowanie prawdopodobie\u0144stwa konwersji terenu na podstawie historycznych zmian?</li> </ul>"},{"location":"projects/meteo_data_for_cities/","title":"Zmiany klimatyczne w miastach - analiza trend\u00f3w w danych z Copernicus Climate Data Store","text":"<p>Opis problemu badawczego: Zmiany klimatu wp\u0142ywaj\u0105 na warunki atmosferyczne w miastach, wzmacniaj\u0105c efekt miejskiej wyspy ciep\u0142a (UHI) i zmieniaj\u0105c wzorce opad\u00f3w czy pr\u0119dko\u015bci wiatru. Celem projektu jest analiza d\u0142ugoterminowych trend\u00f3w wybranych parametr\u00f3w meteorologicznych (np. temperatura, opady, pr\u0119dko\u015b\u0107 wiatru) w wybranych aglomeracjach na podstawie wybranych \u017ar\u00f3de\u0142 danych. Badanie pozwoli okre\u015bli\u0107 tempo zmian oraz ich sezonowo\u015b\u0107, a tak\u017ce por\u00f3wna\u0107 miasta o r\u00f3\u017cnych warunkach klimatycznych.</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>ERA-5 daily statistics - CDS</li> <li>Inne, znalezione samodzielnie</li> </ul> <p>Dodatkowe informacje:</p> <ul> <li>CDS posiada w\u0142asne API</li> <li>ECMWF posiada w\u0142asny projekt earthkit, kt\u00f3ry niby (nie korzysta\u0142em z niego) u\u0142atwia wczytywanie i prac\u0119 z danymi</li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Wst\u0119pne czyszczenie i agregacja danych szereg\u00f3w czasowych (np. u\u015brednianie miesi\u0119czne lub roczne)</li> <li>Eksploracyjna analiza danych (EDA) - wykresy trend\u00f3w w czasie, wykresy korelacji</li> <li>Analiza statystyczna trendu (np. dopasowanie liniowej regresji trendu lub krzywej nieliniowej)</li> <li>Modelowanie szereg\u00f3w czasowych - zastosowanie modeli prognostycznych (np. ARIMA, Prophet lub LSTM) do przewidywania przysz\u0142ych warto\u015bci</li> <li>Ocena niepewno\u015bci prognoz i walidacja modelu na danych historycznych (podzia\u0142 na zbi\u00f3r treningowy i testowy z ostatnich lat)</li> </ul>"},{"location":"projects/other/","title":"Inne","text":""},{"location":"projects/other/#analiza-danych-rastrowych","title":"Analiza danych rastrowych","text":"<p>Przyk\u0142adowe projekty z kt\u00f3rych mo\u017cna czerpa\u0107 inspiracj\u0119:</p> <ul> <li>Segment Anything Model by Qiusheng Wu</li> </ul>"},{"location":"projects/other/#analiza-otwartych-danych-wektorowych","title":"Analiza otwartych danych wektorowych","text":"<p>Jako projekt mo\u017cna r\u00f3wnie\u017c wybra\u0107 interesuj\u0105ce warstwy z otwartych danych wektorowych (Open Street Maps czy Overture Maps) i przeanalizowa\u0107 je w wybrany spos\u00f3b.</p> <p>Przyk\u0142adowe projekty z kt\u00f3rych mo\u017cna czerpa\u0107 inspiracj\u0119:</p> <ul> <li>City-Summit by Kamil Raczycki - takiego typu charakterystyk\u0119 mo\u017cna zrobi\u0107 te\u017c na innych warstwach dost\u0119pnych</li> </ul>"},{"location":"projects/other/#projekty-bardziej-zaawansowane-technicznie-ale-pokrywajace-mniej-obszarow-pracy-z-danymi","title":"Projekty bardziej zaawansowane technicznie, ale pokrywaj\u0105ce mniej obszar\u00f3w pracy z danymi","text":"<p>Istnieje tak\u017ce mo\u017cliwo\u015b\u0107 stworzenia projektu skupiaj\u0105cego si\u0119 na jednym z komponent\u00f3w analizy danych, np. przeszukiwaniu katalog\u00f3w danych, maskowaniu chmur z wykorzystaniem r\u00f3\u017cnych algorytm\u00f3w czy rozproszonym przetwarzaniu danych przestrzennych. W takim wypadku szczeg\u00f3\u0142y zostan\u0105 uzgodnione indywidualnie z grup\u0105.</p>"},{"location":"projects/sr/","title":"Por\u00f3wnanie dost\u0119pnych modeli Super Resolution dla danych Sentinel-2","text":"<p>Opis problemu badawczego: Obrazy satelitarne Sentinel-2 oferuj\u0105 wielospektralne zobrazowania w r\u00f3\u017cnych rozdzielczo\u015bciach (10 m, 20 m, 60 m), ale nie wszystkie pasma maj\u0105 wysok\u0105 rozdzielczo\u015b\u0107 przestrzenn\u0105. To ogranicza ich wykorzystanie w precyzyjnych analizach \u015brodowiskowych, takich jak detekcja zmian, klasyfikacja pokrycia terenu czy monitoring urbanizacji. Celem projektu jest zastosowanie technik Super Resolution (SR) do poprawy jako\u015bci danych Sentinel-2, wykorzystuj\u0105c otwarte modele g\u0142\u0119bokiego uczenia do zwi\u0119kszania rozdzielczo\u015bci obraz\u00f3w.</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>W przypadku Sentinel-2 - Copernicus Data Space Ecosystem / Microsoft Planetary Computer / Earth Search by Element 84 (AWS Registry of Open Data)</li> <li>W przypadku modeli do SR:<ul> <li>DSen2</li> <li>ESRGAN, SRCNN, HighResNet od AllenAI</li> <li>Sentinel-2 Deep Resolution (S2DR2 / S2DR3)</li> <li>Inne</li> </ul> </li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Pobranie obraz\u00f3w Sentinel-2</li> <li>Zastosowanie gotowych modeli</li> <li>Wizualne por\u00f3wnanie i analiza wybranego wska\u017anika spektralnego</li> </ul>"},{"location":"projects/ways_of_working/","title":"Spos\u00f3b pracy nad projektami","text":""},{"location":"projects/ways_of_working/#ogolne-zasady-pracy-nad-projektami","title":"\ud83d\udccc Og\u00f3lne zasady pracy nad projektami","text":"<ol> <li> <p>Wszystkie analizy prowadzimy w Pythonie</p> <ul> <li>Mo\u017cliwe jest zar\u00f3wno budowanie pakietu i uruchamianie analiz przez CLI jak i stworzenie przejrzystych Jupyter Notebooks.</li> <li>\u015arodowisko wirtualne musi by\u0107 odtwarzalne, a wi\u0119c konieczne jest dodanie jego definicji do repozytorium.</li> </ul> </li> <li> <p>Dokumentacja</p> <ul> <li>Ostateczne wyniki projektu powinny zosta\u0107 udost\u0119pnione w formie dokumentacji w GitHub Pages (mo\u017cliwa inna forma).</li> <li>Nale\u017cy prowadzi\u0107 cotygodniowy dziennik (log), gdzie b\u0119d\u0105 2 sekcje: 1) osi\u0105gniety post\u0119p oraz 2) napotkanie problemy / kwestie do przedyskutowania podczas zaj\u0119\u0107.</li> <li>Plik <code>README.md</code> powinien zawiera\u0107 kroki, kt\u00f3re nale\u017cy podj\u0105\u0107 aby uruchomi\u0107 analizy.</li> </ul> </li> <li> <p>Praca podczas zaj\u0119\u0107</p> <ul> <li>Przed ka\u017cdym zaj\u0119ciami nale\u017cy mie\u0107 gotowy log (patrz wy\u017cej), na jego podstawie b\u0119dziemy prowadzi\u0107 dyskusj\u0119 co dalej z projektem.</li> </ul> </li> <li>Pracujemy w grupach 3-osobowych</li> </ol>"},{"location":"projects/ways_of_working/#harmonogram","title":"\ud83d\udcc5 Harmonogram","text":"<pre><code>- Marzec:\n    - 21 - Om\u00f3wienie i wybranie temat\u00f3w\n    - 28 - Przedstawienie wybranych \u017ar\u00f3de\u0142 danych i koncepcji projektu\n- Kwiecie\u0144:\n    - 04 + 11 - Dost\u0119p do danych (pobieranie, wczytywanie)\n    - 25 + 30 - Przygotowane dane do przeprowadzenia analizy (czyszczenie, przygotowanie i eksploracja)\n- Maj:\n    - 09 + 16 + 23 - Modelowanie / analizy statystyczne / ML\n- Czerwiec:\n    - 30 + 06 - Wyniki (wizualizacja)\n    - 13 + 25 - Opracowanie rezultat\u00f3w projektu i wystawienie ocen\n</code></pre>"}]}